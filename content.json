{"meta":{"title":"Voox","subtitle":null,"description":"backend;java;typescript;graphql;fullstack","author":"TJ","url":"https://voox.cc","root":"/"},"pages":[{"title":"","date":"2021-05-07T03:01:42.169Z","updated":"2021-05-07T03:01:42.169Z","comments":false,"path":"404.html","permalink":"https://voox.cc/404.html","excerpt":"","text":"404页面 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} .plan{color: black;background: white;font-size: 30px; margin-top: 20px;} .plan:hover{color: white;background: black;font-size: 30px;} #gg &#123; position: absolute; width: 654px; height: 470px; left: 50%; top: 50%; margin-left: -377px; margin-top: -235px; &#125; &lt;/style&gt; alert('opps, page not found~');"},{"title":"","date":"2021-05-07T03:01:42.213Z","updated":"2021-05-07T03:01:42.213Z","comments":false,"path":"manifest.json","permalink":"https://voox.cc/manifest.json","excerpt":"","text":"{\"short_name\":\"TJ\",\"name\":\"TJ\",\"lang\":\"en\",\"description\":\"TJ's Blog!!\",\"start_url\":\"https://voox.cc\",\"background_color\":\"#00aae7\",\"theme_color\":\"#00aae7\",\"dir\":\"ltr\",\"display\":\"standalone\",\"orientation\":\"any\",\"prefer_related_applications\":false}"},{"title":"About me","date":"2020-03-19T06:59:53.000Z","updated":"2021-05-07T03:01:42.213Z","comments":false,"path":"about/index.html","permalink":"https://voox.cc/about/index.html","excerpt":"","text":"# About Me TJ / 男 2011 年至今 Java 企业级开发经验 熟悉基于 Spring-cloud 微服务架构 熟悉 Nodejs, Docker, RabbitMQ, Redis, Postgres, Typescript, GraphQL 熟悉 Github, Bitbucket 参与过开源项目和开源社区的建设和开发 联系方式Email echo aUB2b294LmNj | base64 -d Github https://github.com/aooppo new Waline({ el: '#waline', path: location.pathname, visitor: true, serverURL: 'https://comments.voox.cc', });"},{"title":"categories","date":"2019-01-31T02:54:24.000Z","updated":"2021-05-07T03:01:42.213Z","comments":false,"path":"categories/index.html","permalink":"https://voox.cc/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-19T06:59:53.000Z","updated":"2021-05-07T03:01:42.213Z","comments":false,"path":"projects/index.html","permalink":"https://voox.cc/projects/index.html","excerpt":"","text":""}],"posts":[{"title":"重新认识NIO","slug":"yq/ywflt3","date":"2021-04-28T12:46:25.000Z","updated":"2021-05-07T03:02:08.119Z","comments":true,"path":"2021/04/28/yq/ywflt3/","link":"","permalink":"https://voox.cc/2021/04/28/yq/ywflt3/","excerpt":"","text":"NIOnon-blocking io 1. 三大组件1.1 Channel &amp; Bufferchannel 有一点类似于 stream，它就是读写数据的双向通道，可以从 channel 中将数据读入 buffer，也可以将 buffer 的数据写入到 channel 中，而之前的 stream 要么是输入要么是输出，stream 是单向通道，channel 相比较于 stream 更为底层。常见的Channel有: FileChannel DatagramChannel SocketChannel ServerSocketChannel buffer 用来缓冲读写数据，常见的 buffer 有 ByteBuffer MappedByteBuffer DirectByteBuffer HeapByteBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer CharBuffer 1.2 SelectorSelector 的作用是什么？选择器提供选择执行已经就绪的任务的能力。从底层来看，Selector 提供了询问通道是否已经准备好执行每个 I/O 操作的能力。Selector 允许单线程处理多个 Channel。仅用单个线程来处理多个 Channels 的好处是，只需要更少的线程来处理通道。事实上，可以只用一个线程处理所有的通道，这样会大量的减少线程之间上下文切换的开销。 在使用 Selector 之前，处理 socket 连接还有以下两种方法 1.2.1 使用多线程技术为每个连接分别开辟一个线程，分别去处理对应的 socket 连接这种方法存在以下几个问题 内存占用高 每个线程都需要占用一定的内存，当连接较多时，会开辟大量线程，导致占用大量内存 线程上下文切换成本高 只适合连接数少的场景 连接数过多，会导致创建很多线程，从而出现问题 1.2.2 使用线程池技术使用线程池，让线程池中的线程去处理连接这种方法存在以下几个问题 阻塞模式下，线程仅能处理一个连接 线程池中的线程获取任务（task）后，只有当其执行完任务之后（断开连接后），才会去获取并执行下一个任务 若 socket 连接一直未断开，则其对应的线程无法处理其他 socket 连接 仅适合短连接场景 短连接即建立连接发送请求并响应后就立即断开，使得线程池中的线程可以快速处理其他连接 1.2.3 使用选择器selector 的作用就是配合一个线程来管理多个 channel（fileChannel 因为是阻塞式的，所以无法使用 selector），获取这些 channel 上发生的事件，这些 channel 工作在非阻塞模式下，当一个 channel 中没有执行任务时，可以去执行其他 channel 中的任务。适合连接数多，但流量较少的场景**若事件未就绪，调用 selector 的 select() 方法会阻塞线程，直到 channel 发生了就绪事件。这些事件就绪后，select 方法就会返回这些事件交给 thread 来处理。 1.2.4 可选择通道(SelectableChannel)并不是所有的 Channel，都是可以被 Selector 复用的。比方说，FileChannel 就不能被选择器复用。为什么呢？判断一个 Channel 能被 Selector 复用，有一个前提：判断他是否继承了一个抽象类 SelectableChannel。如果继承了 SelectableChannel，则可以被复用，否则不能。SelectableChannel 类提供了实现通道的可选择性所需要的公共方法。它是所有支持就绪检查的通道类的父类。所有 socket 通道，都继承了 SelectableChannel 类都是可选择的，包括从管道(Pipe)对象的中获得的通道。而 FileChannel 类，没有继承 SelectableChannel，因此是不是可选通道。通道和选择器注册之后，他们是绑定的关系吗？答案是不是。不是一对一的关系。一个通道可以被注册到多个选择器上，但对每个选择器而言只能被注册一次。通道和选择器之间的关系，使用注册的方式完成。SelectableChannel 可以被注册到 Selector 对象上，在注册的时候，需要指定通道的哪些操作，是 Selector 感兴趣的。 2. ByteBuffer缓冲区(Buffer)就是在内存中预留指定大小的存储空间用来对输入/输出(I/O)的数据作临时存储，这部分预留的内存空间就叫做缓冲区：使用缓冲区有这么两个好处： 减少实际的物理读写次数 缓冲区在创建时就被分配内存，这块内存区域一直被重用，可以减少动态分配和回收内存的次数 ByteBuffer 有几个重要属性 capacity position limit 2.1 ByteBuffer 使用初始化写模式，position 是写入位置，limit 等于容量，下图表示写入 4 个字节后的状态 flip 方法调用后，position 切换为读取位置，limit 切换为读取限制读取 4 个字节后，状态如下 clear 方法调用后，状态如下compact 方法，是把未读完的部分向前压缩，然后切换至写模式 1234567891011121314151617181920@Test void testBuffer() &#123; ByteBuffer buffer = ByteBuffer.allocate(6); buffer.put((byte) 1); buffer.put((byte) 2); buffer.put((byte) 3); buffer.put((byte) 4); buffer.put((byte) 5); buffer.put((byte) 6); // 初始化一个写满的buffer buffer.flip(); // position: 0, limit: 6, capacity: 6 -- 切换为读取模式 buffer.get(); buffer.get(); // position: 2, limit: 6, capacity: 6 -- 读取两个字节后，还剩余四个字节 buffer.compact(); // position: 4, limit: 6, capacity: 6 -- 进行压缩之后将从第五个字节开始 &#125;","categories":[{"name":"netty","slug":"netty","permalink":"https://voox.cc/categories/netty/"},{"name":"nio","slug":"netty/nio","permalink":"https://voox.cc/categories/netty/nio/"}],"tags":[{"name":"netty","slug":"netty","permalink":"https://voox.cc/tags/netty/"},{"name":"nio","slug":"nio","permalink":"https://voox.cc/tags/nio/"}]},{"title":"重置Navicat试用时间","slug":"yq/rauyu5","date":"2021-04-28T02:20:25.000Z","updated":"2021-05-07T03:02:08.127Z","comments":true,"path":"2021/04/28/yq/rauyu5/","link":"","permalink":"https://voox.cc/2021/04/28/yq/rauyu5/","excerpt":"","text":"VersionNavicat Premium 15 (Windows) 1234567891011121314151617181920@echo offecho Delete HKEY_CURRENT_USER\\Software\\PremiumSoft\\NavicatPremium\\Registration15XCSecho waiting......reg delete &quot;HKEY_CURRENT_USER\\Software\\PremiumSoft\\NavicatPremium\\Registration15XCS&quot; /va /fecho.echo Delete Info folder under HKEY_CURRENT_USER\\Software\\Classes\\CLSIDecho waiting......for /f %%i in (&#x27;&quot;REG QUERY &quot;HKEY_CURRENT_USER\\Software\\Classes\\CLSID&quot; /s | findstr /E Info&quot;&#x27;) do ( reg delete %%i /va /f)echo.echo Finishpause","categories":[{"name":"navicat","slug":"navicat","permalink":"https://voox.cc/categories/navicat/"}],"tags":[{"name":"navicat","slug":"navicat","permalink":"https://voox.cc/tags/navicat/"}]},{"title":"bios setting for mac","slug":"yq/oi71k3","date":"2021-04-12T12:46:25.000Z","updated":"2021-05-07T03:02:08.147Z","comments":true,"path":"2021/04/12/yq/oi71k3/","link":"","permalink":"https://voox.cc/2021/04/12/yq/oi71k3/","excerpt":"","text":"#AMD BIOS SettingsNote: Most of these options may not be present in your firmware, we recommend matching up as closely as possible but don’t be too concerned if many of these options are not available in your BIOS#DisableFast BootSecure BootSerial/COM PortParallel PortCompatibility Support Module (CSM)(Must be off, GPU errors like gIO are common when this option in enabled)Special note for 3990X users: macOS currently does not support more than 64 threads in the kernel, and so will kernel panic if it sees more. The 3990X CPU has 128 threads total and so requires half of that disabled. We recommend disabling hyper threading in the BIOS for these situations.#EnableAbove 4G decoding(This must be on, if you can’t find the option then add npci=0x2000 to boot-args. Do not have both this option and npci enabled at the same time.) If you are on a Gigabyte/Aorus or an AsRock motherboard, enabling this option may break certain drivers(ie. Ethernet) and/or boot failures on other OSes, if it does happen then disable this option and opt for npci instead 2020+ BIOS Notes: When enabling Above4G, Resizable BAR Support may become an available on some X570 and newer motherboards. Please ensure this is Disabled instead of set to Auto. EHCI/XHCI Hand-offOS type: Windows 8.1/10 UEFI ModeSATA Mode: AHCI","categories":[{"name":"bios","slug":"bios","permalink":"https://voox.cc/categories/bios/"}],"tags":[{"name":"bios","slug":"bios","permalink":"https://voox.cc/tags/bios/"}]},{"title":"spring-boot 自动装配","slug":"yq/skdzb0","date":"2021-03-30T03:43:55.000Z","updated":"2021-05-07T03:02:08.243Z","comments":true,"path":"2021/03/30/yq/skdzb0/","link":"","permalink":"https://voox.cc/2021/03/30/yq/skdzb0/","excerpt":"","text":"123456@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(DemoApplication.class, args); &#125;&#125; 通过 SpringApplication.run 可以进入方法得知 spring 底层自动扫描所有在META-INF/spring.factories中默认配置好的自动装配类，然后根据条件按需加载组件（bean）到 spring 容器中。 123456789private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; try &#123; Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); &#125;&#125; 123456789package cc.voox.demo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;@SpringBootApplicationpublic class DemoApplication &#123;&#125; @SpringBootApplication 是由多个注解组合而成。 123456@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;&#125; @SpringBootConfiguration 和@Configuration 功能一样 12@Configurationpublic @interface SpringBootConfiguration &#123;&#125; @SpringBootConfiguration is a class-level annotation that is part of the Spring Boot framework. It indicates that a class provides application configuration. Spring Boot favors Java-based configuration. As a result, the @SpringBootConfiguration annotation is the primary source for configuration in applications. @ComponentScan 包扫描器 1234@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123;&#125; 自动导入AutoConfigurationImportSelector 组件后，通过selectImports方法封装所有满足的类成AutoConfigurationEntry进行导入 123456@Overridepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123; AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());&#125; 1234567891011121314protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);&#125; 利用PackageImports类批量注册组件到容器中。 123456789101112@Import(AutoConfigurationPackages.Registrar.class)public @interface AutoConfigurationPackage &#123;&#125;static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; register(registry, new PackageImports(metadata).getPackageNames().toArray(new String[0])); &#125;&#125;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://voox.cc/tags/java/"},{"name":"spring","slug":"spring","permalink":"https://voox.cc/tags/spring/"}]},{"title":"Netty模型","slug":"Netty模型","date":"2020-10-29T09:00:25.000Z","updated":"2021-05-07T03:01:42.185Z","comments":true,"path":"2020/10/29/Netty模型/","link":"","permalink":"https://voox.cc/2020/10/29/Netty%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"提及Netty模型，不得不从Reactor 模式说起， Reactor模式是基于事件驱动，特别适合处理海量的 I/O 事件。 Reactor 模式下，又细分为： 单线程模型 多线程模型 主从多线程模型 单线程模型Reactor 单线程模型，指的是所有的 IO 操作都在同一个 NIO 线程上面完成，NIO 线程的职责如下： 作为 NIO 服务端，接收客户端的 TCP 连接； 作为 NIO 客户端，向服务端发起 TCP 连接； 读取通信对端的请求或者应答消息； 向通信对端发送消息请求或者应答消息。 Reactor 单线程模型示意图如下所示： 由于 Reactor 模式使用的是异步非阻塞 IO，所有的 IO 操作都不会导致阻塞，理论上一个线程可以独立处理所有 IO 相关的操作。从架构层面看，一个 NIO 线程确实可以完成其承担的职责。例如，通过 Acceptor 类接收客户端的 TCP 连接请求消息，链路建立成功之后，通过 Dispatch 将对应的 ByteBuffer 派发到指定的 Handler 上进行消息解码。用户线程可以通过消息编码通过 NIO 线程将消息发送给客户端。 对于一些小容量应用场景，可以使用单线程模型。但是对于高负载、大并发的应用场景却不合适，主要原因如下： 一个 NIO 线程同时处理成百上千的链路，性能上无法支撑，即便 NIO 线程的 CPU 负荷达到 100%，也无法满足海量消息的编码、解码、读取和发送； 当 NIO 线程负载过重之后，处理速度将变慢，这会导致大量客户端连接超时，超时之后往往会进行重发，这更加重了 NIO 线程的负载，最终会导致大量消息积压和处理超时，成为系统的性能瓶颈； 可靠性问题：一旦 NIO 线程意外跑飞，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。 为了解决这些问题，演进出了 Reactor 多线程模型，下面我们一起学习下 Reactor 多线程模型。 多线程模型Rector 多线程模型与单线程模型最大的区别就是有一组 NIO 线程处理 IO 操作，它的原理图如下： Reactor 多线程模型的特点： 有专门一个 NIO 线程 -Acceptor 线程用于监听服务端，接收客户端的 TCP 连接请求； 网络 IO 操作 - 读、写等由一个 NIO 线程池负责，线程池可以采用标准的 JDK 线程池实现，它包含一个任务队列和 N 个可用的线程，由这些 NIO 线程负责消息的读取、解码、编码和发送； 1 个 NIO 线程可以同时处理 N 条链路，但是 1 个链路只对应 1 个 NIO 线程，防止发生并发操作问题。 在绝大多数场景下，Reactor 多线程模型都可以满足性能需求；但是，在极个别特殊场景中，一个 NIO 线程负责监听和处理所有的客户端连接可能会存在性能问题。例如并发百万客户端连接，或者服务端需要对客户端握手进行安全认证，但是认证本身非常损耗性能。在这类场景下，单独一个 Acceptor 线程可能会存在性能不足问题，为了解决性能问题，产生了第三种 Reactor 线程模型 - 主从 Reactor 多线程模型。 主从多线程模型主从 Reactor 线程模型的特点是：服务端用于接收客户端连接的不再是个 1 个单独的 NIO 线程，而是一个独立的 NIO 线程池。Acceptor 接收到客户端 TCP 连接请求处理完成后（可能包含接入认证等），将新创建的 SocketChannel 注册到 IO 线程池（sub reactor 线程池）的某个 IO 线程上，由它负责 SocketChannel 的读写和编解码工作。Acceptor 线程池仅仅只用于客户端的登陆、握手和安全认证，一旦链路建立成功，就将链路注册到后端 subReactor 线程池的 IO 线程上，由 IO 线程负责后续的 IO 操作。 它的线程模型如下图所示： 利用主从 NIO 线程模型，可以解决 1 个服务端监听线程无法有效处理所有客户端连接的性能不足问题。 它的工作流程总结如下： 从主线程池中随机选择一个 Reactor 线程作为 Acceptor 线程，用于绑定监听端口，接收客户端连接； Acceptor 线程接收客户端连接请求之后创建新的 SocketChannel，将其注册到主线程池的其它 Reactor 线程上，由其负责接入认证、IP 黑白名单过滤、握手等操作； 步骤 2 完成之后，业务层的链路正式建立，将 SocketChannel 从主线程池的 Reactor 线程的多路复用器上摘除，重新注册到 Sub 线程池的线程上，用于处理 I/O 的读写操作。 Netty 线程模型服务端线程模型一种比较流行的做法是服务端监听线程和 IO 线程分离，类似于 Reactor 的多线程模型，它的工作原理图如下： 下面我们结合 Netty 的源码，对服务端创建线程工作流程进行介绍： 12345678910111213141516171819202122232425public static void main(String[] args) throws InterruptedException &#123; NioEventLoopGroup bossGroup = new NioEventLoopGroup(); NioEventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new ServerHandler()); &#125; &#125;); System.out.println(&quot;server is up!&quot;); ChannelFuture channelFuture = serverBootstrap.bind(6668).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125;&#125; 通常情况下，服务端的创建是在用户进程启动的时候进行，因此一般由 Main 函数或者启动类负责创建，服务端的创建由业务线程负责完成。在创建服务端的时候实例化了 2 个 EventLoopGroup，1 个 EventLoopGroup 实际就是一个 EventLoop 线程组，负责管理 EventLoop 的申请和释放。 EventLoopGroup 管理的线程数可以通过构造函数设置，如果没有设置，默认取 -Dio.netty.eventLoopThreads，如果该系统参数也没有指定，则为可用的 CPU 内核数 × 2。 123456789101112public abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup &#123; private static final int DEFAULT_EVENT_LOOP_THREADS; static &#123; DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;-Dio.netty.eventLoopThreads: &#123;&#125;&quot;, DEFAULT_EVENT_LOOP_THREADS); &#125; &#125; bossGroup 线程组实际就是 Acceptor 线程池，负责处理客户端的 TCP 连接请求，如果系统只有一个服务端端口需要监听，则建议 bossGroup 线程组线程数设置为 1。 workerGroup 是真正负责 I/O 读写操作的线程组，通过 ServerBootstrap 的 group 方法进行设置，用于后续的 Channel 绑定。 抽象出NioEventLoop来表示一个不断循环执行处理任务的线程，每个NioEventLoop有一个selector，用于监听绑定在其上的socket链路。 1、串行化设计避免线程竞争netty采用串行化设计理念，从消息的读取-&gt;解码-&gt;处理-&gt;编码-&gt;发送，始终由IO线程NioEventLoop负责。整个流程不会进行线程上下文切换，数据无并发修改风险。 一个NioEventLoop聚合一个多路复用器selector，因此可以处理多个客户端连接。 netty只负责提供和管理“IO线程”，其他的业务线程模型由用户自己集成。 时间可控的简单业务建议直接在“IO线程”上处理，复杂和时间不可控的业务建议投递到后端业务线程池中处理。 2、定时任务与时间轮NioEventLoop中的Thread线程按照时间轮中的步骤不断循环执行： a)在时间片Tirck内执行selector.select()轮询监听IO事件； b)处理监听到的就绪IO事件； c)执行任务队列taskQueue/delayTaskQueue中的非IO任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081@Overrideprotected void run() &#123; for (;;) &#123; try &#123; switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) &#123; case SelectStrategy.CONTINUE: continue; case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); // &#x27;wakenUp.compareAndSet(false, true)&#x27; is always evaluated // before calling &#x27;selector.wakeup()&#x27; to reduce the wake-up // overhead. (Selector.wakeup() is an expensive operation.) // // However, there is a race condition in this approach. // The race condition is triggered when &#x27;wakenUp&#x27; is set to // true too early. // // &#x27;wakenUp&#x27; is set to true too early if: // 1) Selector is waken up between &#x27;wakenUp.set(false)&#x27; and // &#x27;selector.select(...)&#x27;. (BAD) // 2) Selector is waken up between &#x27;selector.select(...)&#x27; and // &#x27;if (wakenUp.get()) &#123; ... &#125;&#x27;. (OK) // // In the first case, &#x27;wakenUp&#x27; is set to true and the // following &#x27;selector.select(...)&#x27; will wake up immediately. // Until &#x27;wakenUp&#x27; is set to false again in the next round, // &#x27;wakenUp.compareAndSet(false, true)&#x27; will fail, and therefore // any attempt to wake up the Selector will fail, too, causing // the following &#x27;selector.select(...)&#x27; call to block // unnecessarily. // // To fix this problem, we wake up the selector again if wakenUp // is true immediately after selector.select(...). // It is inefficient in that it wakes up the selector for both // the first case (BAD - wake-up required) and the second case // (OK - no wake-up required). if (wakenUp.get()) &#123; selector.wakeup(); &#125; // fall through default: &#125; cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; if (ioRatio == 100) &#123; try &#123; processSelectedKeys(); &#125; finally &#123; // Ensure we always run tasks. runAllTasks(); &#125; &#125; else &#123; final long ioStartTime = System.nanoTime(); try &#123; processSelectedKeys(); &#125; finally &#123; // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); &#125; &#125; &#125; catch (Throwable t) &#123; handleLoopException(t); &#125; // Always handle shutdown even if the loop processing threw an exception. try &#123; if (isShuttingDown()) &#123; closeAll(); if (confirmShutdown()) &#123; return; &#125; &#125; &#125; catch (Throwable t) &#123; handleLoopException(t); &#125; &#125;&#125; NioEventLoop与NioChannel类关系 一个NioEventLoopGroup下包含多个NioEventLoop 每个NioEventLoop中包含有一个Selector，一个taskQueue，一个delayedTaskQueue 每个NioEventLoop的Selector上可以注册监听多个AbstractNioChannel 每个AbstractNioChannel只会绑定在唯一的NioEventLoop上 每个AbstractNioChannel都绑定有一个自己的DefaultChannelPipeline","categories":[],"tags":[{"name":"netty","slug":"netty","permalink":"https://voox.cc/tags/netty/"},{"name":"nio","slug":"nio","permalink":"https://voox.cc/tags/nio/"}]},{"title":"Linux五种IO模型","slug":"linux五种IO模型","date":"2020-10-27T02:25:21.000Z","updated":"2021-05-07T03:01:42.201Z","comments":true,"path":"2020/10/27/linux五种IO模型/","link":"","permalink":"https://voox.cc/2020/10/27/linux%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Linux下主要的IO主要分为：阻塞IO(Blocking IO)，非阻塞IO(Non-blocking IO)，同步IO(Sync IO)和异步IO(Async IO)。同步：调用端会一直等待服务端响应，直到返回结果。异步：调用端发起调用之后不会立刻返回，不会等待服务端响应。服务端通过通知机制或者回调函数来通知客户端。阻塞：服务端返回结果之前，客户端线程会被挂起，此时线程不可被CPU调度，线程暂停运行。非阻塞：在服务端返回前，函数不会阻塞调用端线程，而会立刻返回。同步异步的区别在于：服务端在拷贝数据时是否阻塞调用端线程；阻塞和非阻塞的区别在于：调用端线程在调用function后是否立刻返回。要理解这些I/O，需要先理解一些基本的概念。 用户态和核心态Linux系统中分为核心态(Kernel model)和用户态(User model)，CPU会在两个model之间切换。 核心态代码拥有完全的底层资源控制权限，可以执行任何CPU指令，访问任何内存地址，其占有的处理机是不允许被抢占的。内核态的指令包括：启动I/O，内存清零，修改程序状态字，设置时钟，允许/终止中断和停机。内核态的程序崩溃会导致PC停机。 用户态是用户程序能够使用的指令，不能直接访问底层硬件和内存地址。用户态运行的程序必须委托系统调用来访问硬件和内存。用户态的指令包括：控制转移，算数运算，取数指令，访管指令（使用户程序从用户态陷入内核态）。 用户态和核心态的切换用户态切换到核心态有三种方式： a.系统调用这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 b.异常当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 c.外围设备的中断当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。 进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 进程阻塞 正在执行的进程由于一些事情发生，如请求资源失败、等待某种操作完成、新数据尚未达到或者没有新工作做等，由系统自动执行阻塞原语，使进程状态变为阻塞状态。因此，进程阻塞是进程自身的一种主动行为，只有处于运行中的进程才可以将自身转化为阻塞状态。当进程被阻塞，它是不占用CPU资源的。 文件描述符(fd, File Descriptor) FD用于描述指向文件的引用的抽象化概念。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存I/O 缓存IO又被称作标准IO，大多数文件系统的默认IO 操作都是缓存IO。在Linux的缓存IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存IO的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 Linux下的五种I/O模型 Linux下主要有以下五种I/O模型： 阻塞I/O（blocking IO） 非阻塞I/O (nonblocking I/O) I/O 复用 (I/O multiplexing) 信号驱动I/O (signal driven I/O (SIGIO)) 异步I/O (asynchronous I/O) 阻塞IO模型进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。数据准备好后，从内核拷贝到用户空间，IO函数返回成功指示。阻塞IO模型图如下所示： 非阻塞IO模型通过进程反复调用IO函数，在数据拷贝过程中，进程是阻塞的。模型图如下所示: IO复用模型主要是select和epoll。一个线程可以对多个IO端口进行监听，当socket有读写事件时分发到具体的线程进行处理。模型如下所示： 信号驱动IO模型信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。过程如下图所示： 异步IO模型相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。异步过程如下图所示： 五种IO模型比较阻塞IO和非阻塞IO的区别调用阻塞IO后进程会一直等待对应的进程完成，而非阻塞IO不会等待对应的进程完成，在kernel还在准备数据的情况下直接返回。 同步IO和异步IO的区别首先看一下POSIX中对这两个IO的定义： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。注意到non-blocking IO会一直轮询(polling)，这个过程是没有阻塞的，但是recvfrom阶段blocking IO,non-blocking IO和IO multiplexing都是阻塞的。而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 IO复用之select、poll、epoll简介 epoll是linux所特有，而select是POSIX所规定，一般操作系统均有实现。 selectselect本质是通过设置或检查存放fd标志位的数据结构来进行下一步处理。缺点是： 单个进程可监视的fd数量被限制，即能监听端口的大小有限。一般来说和系统内存有关，具体数目可以cat /proc/sys/fs/file-max察看。32位默认是1024个，64位默认为2048个 对socket进行扫描时是线性扫描，即采用轮询方法，效率低。当套接字比较多的时候，每次select()都要遍历FD_SETSIZE个socket来完成调度，不管socket是否活跃都遍历一遍。会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，就避免了轮询，这正是epoll与kqueue做的 需要维护一个用来存放大量fd的数据结构，会使得用户空间和内核空间在传递该结构时复制开销大 pollpoll本质和select相同，将用户传入的数据拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或主动超时，被唤醒后又要再次遍历fd。它没有最大连接数的限制，原因是它是基于链表来存储的，但缺点是： 大量的fd的数组被整体复制到用户态和内核空间之间，不管有无意义。 poll还有一个特点“水平触发”，如果报告了fd后，没有被处理，那么下次poll时再次报告该ffd。 epollepoll支持水平触发和边缘触发，最大特点在于边缘触发，只告诉哪些fd刚刚变为就绪态，并且只通知一次。还有一特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一量该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。epoll的优点： 没有最大并发连接的限制。 效率提升，只有活跃可用的FD才会调用callback函数。 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递。 select、poll、epoll区别总结： - 支持一个进程打开连接数 IO效率 消息传递方式 select 32位机器1024个，64位2048个 IO效率低 内核需要将消息传递到用户空间，都需要内核拷贝动作 poll 无限制，原因基于链表存储 IO效率低 内核需要将消息传递到用户空间，都需要内核拷贝动作 epoll 有上限，但很大，2G内存20W左右 只有活跃的socket才调用callback，IO效率高 通过内核与用户空间共享一块内存来实现 [参考资料] Unix网络编程 File descriptor","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://voox.cc/tags/linux/"},{"name":"io","slug":"io","permalink":"https://voox.cc/tags/io/"}]},{"title":"Linux 用户空间与内核空间","slug":"Linux 用户空间与内核空间","date":"2020-10-23T14:12:32.000Z","updated":"2021-05-07T03:01:42.181Z","comments":true,"path":"2020/10/23/Linux 用户空间与内核空间/","link":"","permalink":"https://voox.cc/2020/10/23/Linux%20%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E4%B8%8E%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/","excerpt":"","text":"linux中的用户空间与内核空间 在操作系统中，虚拟内存通常会被分成用户空间（英语：User space，又译为使用者空间），与核心空间（英语：Kernel space，又译为内核空间）这两个区块。这是存储器保护机制中的一环。内核、核心扩充（kernel extensions）、以及驱动程序，运行在核心空间上。而其他的应用程序，则运行在用户空间上。所有运行在用户空间的应用程序，都被统称为用户级（userland）。 学习 Linux 时，经常可以看到两个词：User space（用户空间）和 Kernel space（内核空间）。 简单说，Kernel space 是 Linux 内核的运行空间，User space 是用户程序的运行空间。为了安全，它们是隔离的，即使用户的程序崩溃了，内核也不受影响。 Linux简化了分段机制，使得虚拟地址与线性地址总是一致，因此，Linux的虚拟地址空间也为0～4G。Linux内核将这4G字节的空间分为两部分：将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为“内核空间”；而将较低的3G字节（从虚拟地址 0x00000000到0xBFFFFFFF），供各个进程使用，称为“用户空间 ）。 Kernel space 可以执行任意命令，调用系统的一切资源；User space 只能执行简单的运算，不能直接调用系统资源，必须通过系统接口（又称 system call），才能向内核发出指令。 12345str = &quot;my string&quot; // 用户空间x = x + 1file.write(str) // 切换到内核空间y = x + 2 // 切换回用户空间 上面代码中，第一行和第二行都是简单的赋值运算，在 User space 执行。第三行需要写入文件，就要切换到 Kernel space，因为用户不能直接写文件，必须通过内核安排。第四行又是赋值运算，就切换回 User space。 查看 CPU 时间在 User space 与 Kernel Space 之间的分配情况，可以使用top命令。它的第三行输出就是 CPU 时间分配统计。 其中，第一项0.0 us（user 的缩写）就是 CPU 消耗在 User space 的时间百分比，第二项3 .2 sy（system 的缩写）是消耗在 Kernel space 的时间百分比。 其他 6 个指标的含义如下: ni：niceness 的缩写，CPU 消耗在 nice 进程（低优先级）的时间百分比 id：idle 的缩写，CPU 消耗在闲置进程的时间百分比，这个值越低，表示 CPU 越忙 wa：wait 的缩写，CPU 等待外部 I/O 的时间百分比，这段时间 CPU 不能干其他事，但是也没有执行运算，这个值太高就说明外部设备有问题 hi：hardware interrupt 的缩写，CPU 响应硬件中断请求的时间百分比 si：software interrupt 的缩写，CPU 响应软件中断请求的时间百分比 st：stole time 的缩写，该项指标只对虚拟机有效，表示分配给当前虚拟机的 CPU 时间之中，被同一台物理机上的其他虚拟机偷走的时间百分比 如果想查看单个程序的耗时，一般使用time命令。 程序名之前加上time命令，会在程序执行完毕以后，默认显示三行统计。 real：程序从开始运行到结束的全部时间，这是用户能感知到的时间，包括 CPU 切换去执行其他任务的时间。 user：程序在 User space 执行的时间 sys：程序在 Kernel space 执行的时间 user和sys之和，一般情况下，应该小于real。但如果是多核 CPU，这两个指标反映的是所有 CPU 的总耗时，所以它们之和可能大于real。 [参考资料]User space 与 Kernel spaceUser space","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://voox.cc/tags/linux/"},{"name":"karnel","slug":"karnel","permalink":"https://voox.cc/tags/karnel/"}]},{"title":"kubesphere 安装与部署","slug":"kubesphere-安装与部署","date":"2020-10-13T07:39:19.000Z","updated":"2021-05-07T03:01:42.197Z","comments":true,"path":"2020/10/13/kubesphere-安装与部署/","link":"","permalink":"https://voox.cc/2020/10/13/kubesphere-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/","excerpt":"","text":"前提条件 Kubernetes版本： 1.15.x ≤ K8s version ≤ 1.17.x；Helm版本： 2.10.0 ≤ Helm Version ＜ 3.0.0，建议使用 Helm 2.16.2（不支持 helm 2.16.0 #6894），且已安装了 Tiller，参考 如何安装与配置 Helm （预计 3.0 支持 Helm v3）；集群已有默认的存储类型（StorageClass），若还没有准备存储请参考 安装 OpenEBS 创建 LocalPV 存储类型 用作开发测试环境。集群能够访问外网，若无外网请参考 在 Kubernetes 离线安装 KubeSphere。 All-in-One 模式对于首次接触 KubeSphere 的用户，想寻找一个最快安装和体验 KubeSphere 的方式，all-in-one 模式可一键安装 KubeSphere 和 Kubernetes 至一台目标机器。 KubeSphere 2.1 默认仅开启最小化安装，Installer 已支持自定义安装各个可插拔的功能组件，用户可根据业务需求和机器配置选择安装所需的组件，请确保开启可插拔组件之前机器资源满足最低要求，参考 安装说明 开启可选组件的安装。 若您的机器资源配置充足（CPU 不小于 8 核，内存不小于 16 G），非常建议您在安装前 将 KubeSphere 所有功能组件都开启 后再执行安装，体验 KubeSphere 容器平台端到端完整的容器管理与运维能力。 安装时间跟网络情况和带宽、机器配置、安装节点个数等因素有关，可通过调高带宽的方式，或在安装前 配置镜像加速器 来加快安装速度。 1234567891011121314151617181920212223vi helm-rbac.yaml#### helm-rbac.yaml startapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system #### helm-rbac.yaml end 12345# 需要开启代理curl -L https://git.io/get_helm.sh | bash#关闭代理后执行kubectl apply -f helm-rbac.yaml 123helm init --service-account=tiller -- tiller-image=sapcc/tiller:v2.16.3 --history-max 300# 给master k8s-node1 去除污点 kubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule- 123kubectl create ns openebs# 安装openebs 作为默认存储 helm install --namespace openebs --name openebs stable/openebs --version 1.5.0 12# 设置默认StorageClasskubectl patch storageclass openebs-hostpath -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27; 12#给master k8s-node1 添加污点kubectl taint nodes master1 node-role.kubernetes.io/master=:NoSchedule 至此，StorageClass 安装完毕 部署 KubeSphere最小化快速部署1kubectl apply -f https://raw.githubusercontent.com/kubesphere/ks-installer/v2.1.1/deploy/kubesphere-installer.yaml Inspect the logs of installation. 1kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f admin/P@88w0rd 登录 12#添加污点kubectl taint nodes k8s-node1 node-role.kubernetes.io/master=:NoSchedule","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"https://voox.cc/tags/k8s/"},{"name":"k8s-cluster","slug":"k8s-cluster","permalink":"https://voox.cc/tags/k8s-cluster/"}]},{"title":"K8s Devops环境搭建准备","slug":"K8s部署环境准备","date":"2020-10-12T09:26:35.000Z","updated":"2021-05-07T03:01:42.177Z","comments":true,"path":"2020/10/12/K8s部署环境准备/","link":"","permalink":"https://voox.cc/2020/10/12/K8s%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"1.安装依赖 vagrant 和virtual box创建文件 vagrantfile以下是文件内容： 123456789101112131415161718192021222324252627Vagrant.configure(&quot;2&quot;) do |config| (1..3).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;private_network&quot;, ip: &quot;192.168.56.#&#123;99+i&#125;&quot;, netmask: &quot;255.255.255.0&quot; # 设置主机与虚拟机的共享目录 # node.vm.synced_folder &quot;~/Documents/vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 4 end end endend 然后执行 vagrant up 设置每一个虚拟机的网络环境 连接方式需要变成NAT网络每一个虚拟机的mac 地址需要刷新重新生成 12345678# 检查eth0 ipip route show# 确认eth0 对应的ip 在每个虚拟机不一样ip addr# 测试能否联通其他虚拟机ping 10.0.2.5#测试能否联通外网ping baidu.com 进入每一台虚拟机 开启密码访问登录 12345678910vagrant ssh k8s-node1su rootvi /etc/ssh/sshd_config# 输入i 进入编辑模式# 更改 PasswordAuthentication no 设成yes# 方便远程可以通过用户密码的方式登录PasswordAuthentication yes#编辑后按ESC 输入wq 保存并退出# 重启sshd 服务 让其生效service sshd restart 通过ssh 登录各个虚拟机 1234567# 关闭防火墙systemctl stop firewalld# 禁用防火墙systemctl disable firewalld#关闭linux安全策略sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/configsetenforce 0 12345#（1）临时关闭swap分区, 重启失效;swapoff -a#（2）永久关闭swap分区sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab 添加主机名与ip对应关系 12345678vi /etc/hosts10.0.2.5 k8s-node110.0.2.4 k8s-node210.0.2.15 k8s-node3# 如果主机名没设置对hostnamectl set-hostname &lt;newhostname&gt; 1234567# 桥接的ipv4 流量传递到iptablescat &gt;/etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables =1net.bridge.bridge-nf-call-iptables =1EOFsysctl --system 2. k8s-集群搭建-安装Docker、kubelet、kubeadm、kubectl123456789101112131415161718192021222324252627282930# 卸载原来的dockersudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine# 安装依赖sudo yum update -y &amp;&amp; sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # 添加官方yum库sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装dockersudo yum install docker-ce docker-ce-cli containerd.io# 查看docker版本docker --version# 开机启动systemctl enable --now docker 添加docker加速 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://yourcode.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 1234567891011#更换yum 源为aliyuncat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 123yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3systemctl enable kubeletsystemctl start kubelet 3. k8s master节点安装1234567kubeadm init \\ --apiserver-advertise-address 10.0.2.5 \\ --cert-dir /etc/kubernetes/pki \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version 1.17.3 \\ --pod-network-cidr 10.244.0.0/16 \\ --service-cidr 10.96.0.0/16 \\ 在master节点执行文件， 执行文件前设置文件可执行(700) 1234567891011121314151617#!/bin/bashimages=( kube-apiserver:v1.17.3 kube-proxy:v1.17.3 kube-controller-manager:v1.17.3 kube-scheduler:v1.17.3 coredns:1.6.5 etcd:3.4.3-0 pause:3.1)for imageName in $&#123;images[@]&#125; ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageNamedone 123mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 添加一个网络在master node将下面抽取成一个yml文件执行kubectl apply -f xxxx.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [&#x27;NET_ADMIN&#x27;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: &#x27;RunAsAny&#x27;---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelrules: - apiGroups: [&#x27;extensions&#x27;] resources: [&#x27;podsecuritypolicies&#x27;] verbs: [&#x27;use&#x27;] resourceNames: [&#x27;psp.flannel.unprivileged&#x27;] - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | &#123; &quot;name&quot;: &quot;cbr0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ &#123; &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: &#123; &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true &#125; &#125;, &#123; &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: &#123; &quot;portMappings&quot;: true &#125; &#125; ] &#125; net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - amd64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - arm64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - arm hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - ppc64le hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - s390x hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 在node 执行加入master 12kubeadm join 10.0.2.5:6443 --token vi3lq8.vze5hodae0oufto2 \\ --discovery-token-ca-cert-hash sha256:25518d08b8a03076f808200b2de973b089c7b04b877d9fbc51a9c5139d141180 至此, k8s 集群的基础环境搭建完成。接下来会介绍如何搭建kubesphere。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"https://voox.cc/tags/k8s/"},{"name":"k8s-cluster","slug":"k8s-cluster","permalink":"https://voox.cc/tags/k8s-cluster/"}]},{"title":"Sentinel 的使用","slug":"Sentinel-的使用","date":"2020-10-09T09:30:37.000Z","updated":"2021-05-07T03:01:42.197Z","comments":true,"path":"2020/10/09/Sentinel-的使用/","link":"","permalink":"https://voox.cc/2020/10/09/Sentinel-%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"引入 Sentinel 依赖如果您的应用使用了 Maven，则在 pom.xml 文件中加入以下代码即可：1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 下面这个例子就是一个最简单的使用 Sentinel 的例子: 12345678910111213141516171819202122232425262728@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceApplication.class, args); &#125;&#125;@Servicepublic class TestService &#123; @SentinelResource(value = &quot;sayHello&quot;) public String sayHello(String name) &#123; return &quot;Hello, &quot; + name; &#125;&#125;@RestControllerpublic class TestController &#123; @Autowired private TestService service; @GetMapping(value = &quot;/hello/&#123;name&#125;&quot;) public String apiHello(@PathVariable String name) &#123; return service.sayHello(name); &#125;&#125; @SentinelResource 注解用来标识资源是否被限流、降级。上述例子上该注解的属性 sayHello 表示资源名。 @SentinelResource 还提供了其它额外的属性如 blockHandler，blockHandlerClass，fallback 用于表示限流或降级的操作（注意有方法签名要求），更多内容可以参考 Sentinel 注解支持文档。若不配置 blockHandler、fallback 等函数，则被流控降级时方法会直接抛出对应的 BlockException；若方法未定义 throws BlockException 则会被 JVM 包装一层 UndeclaredThrowableException。 注：一般推荐将 @SentinelResource 注解加到服务实现上，而在 Web 层直接使用 Spring Cloud Alibaba 自带的 Web 埋点适配。Sentinel Web 适配同样支持配置自定义流控处理逻辑，参考 相关文档。 Sentinel 控制台Sentinel 控制台提供一个轻量级的控制台，它提供机器发现、单机资源实时监控、集群资源汇总，以及规则管理的功能。您只需要对应用进行简单的配置，就可以使用这些功能。 注意: 集群资源汇总仅支持 500 台以下的应用集群，有大概 1 - 2 秒的延时。 Figure 1. Sentinel Dashboard开启该功能需要3个步骤： 获取控制台您可以从 release 页面 下载最新版本的控制台 jar 包。 您也可以从最新版本的源码自行构建 Sentinel 控制台： 下载 控制台 工程 使用以下命令将代码打包成一个 fat jar: mvn clean package 启动控制台Sentinel 控制台是一个标准的 Spring Boot 应用，以 Spring Boot 的方式运行 jar 包即可。 12java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 如若8080端口冲突，可使用 -Dserver.port=新端口 进行设置 配置控制台信息application.yml 123456spring: cloud: sentinel: transport: port: 8719 dashboard: localhost:8080 这里的 spring.cloud.sentinel.transport.port 端口配置会在应用对应的机器上启动一个 Http Server，该 Server 会与 Sentinel 控制台做交互。比如 Sentinel 控制台添加了一个限流规则，会把规则数据 push 给这个 Http Server 接收，Http Server 再将规则注册到 Sentinel 中。 更多 Sentinel 控制台的使用及问题参考： Sentinel 控制台文档 以及 Sentinel FAQ Feign 支持Sentinel 适配了 Feign 组件。如果想使用，除了引入 spring-cloud-starter-alibaba-sentinel 的依赖外还需要 2 个步骤： 配置文件打开 Sentinel 对 Feign 的支持：feign.sentinel.enabled=true 加入 spring-cloud-starter-openfeign 依赖使 Sentinel starter 中的自动化配置类生效： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 这是一个 FeignClient 的简单使用示例： 12345678910111213141516171819@FeignClient(name = &quot;service-provider&quot;, fallback = EchoServiceFallback.class, configuration = FeignConfiguration.class)public interface EchoService &#123; @RequestMapping(value = &quot;/echo/&#123;str&#125;&quot;, method = RequestMethod.GET) String echo(@PathVariable(&quot;str&quot;) String str);&#125;class FeignConfiguration &#123; @Bean public EchoServiceFallback echoServiceFallback() &#123; return new EchoServiceFallback(); &#125;&#125;class EchoServiceFallback implements EchoService &#123; @Override public String echo(@PathVariable(&quot;str&quot;) String str) &#123; return &quot;echo fallback&quot;; &#125;&#125; Feign 对应的接口中的资源名策略定义：httpmethod:protocol://requesturl。@FeignClient 注解中的所有属性，Sentinel 都做了兼容EchoService 接口中方法 echo 对应的资源名为 GET:http://service-provider/echo/{str}。","categories":[],"tags":[]},{"title":"从0到1手写一个RPC实现","slug":"从0到1手写一个RPC实现","date":"2020-10-08T05:35:36.000Z","updated":"2021-05-07T03:01:42.213Z","comments":true,"path":"2020/10/08/从0到1手写一个RPC实现/","link":"","permalink":"https://voox.cc/2020/10/08/%E4%BB%8E0%E5%88%B01%E6%89%8B%E5%86%99%E4%B8%80%E4%B8%AARPC%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"计划从0到1手写一个RPC的实现我准备把具体实现和想法记录下来，对学习其他RPC框架有一个参考和比较。可能用到的技术栈： spring netty kryo, protobuf, thrift, hessian (序列化和反序列化) zookeeper, etcd, redis (服务注册和发现)","categories":[],"tags":[{"name":"rpc","slug":"rpc","permalink":"https://voox.cc/tags/rpc/"},{"name":"java","slug":"java","permalink":"https://voox.cc/tags/java/"}]},{"title":"AQS","slug":"AQS","date":"2020-10-08T04:48:48.000Z","updated":"2021-05-07T03:01:42.169Z","comments":true,"path":"2020/10/08/AQS/","link":"","permalink":"https://voox.cc/2020/10/08/AQS/","excerpt":"","text":"1 ReentrantLock1.1 ReentrantLock特性概览ReentrantLock意思为可重入锁，指的是一个线程能够对一个临界资源重复加锁。为了帮助大家更好地理解ReentrantLock的特性，我们先将ReentrantLock跟常用的Synchronized进行比较，其特性如下: 1234567891011121314151617181920212223242526272829// **************************Synchronized的使用方式**************************// 1.用于代码块synchronized (this) &#123;&#125;// 2.用于对象synchronized (object) &#123;&#125;// 3.用于方法public synchronized void test () &#123;&#125;// 4.可重入for (int i = 0; i &lt; 100; i++) &#123; synchronized (this) &#123;&#125;&#125;// **************************ReentrantLock的使用方式**************************public void test () throw Exception &#123; // 1.初始化选择公平锁、非公平锁 ReentrantLock lock = new ReentrantLock(true); // 2.可用于代码块 lock.lock(); try &#123; try &#123; // 3.支持多种加锁方式，比较灵活; 具有可重入特性 if(lock.tryLock(100, TimeUnit.MILLISECONDS))&#123; &#125; &#125; finally &#123; // 4.手动释放锁 lock.unlock() &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 1.2 ReentrantLock与AQS的关联非公平锁源码中的加锁流程如下： 12345678910111213// java.util.concurrent.locks.ReentrantLock#NonfairSync// 非公平锁static final class NonfairSync extends Sync &#123; ... final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; ...&#125; 这块代码的含义为： 若通过CAS设置变量State（同步状态）成功，也就是获取锁成功，则将当前线程设置为独占线程。 若通过CAS设置变量State（同步状态）失败，也就是获取锁失败，则进入Acquire方法进行后续处理。 第一步很好理解，但第二步获取锁失败后，后续的处理策略是怎么样的呢？这块可能会有以下思考： 某个线程获取锁失败的后续流程是什么呢？有以下两种可能：(1) 将当前线程获锁结果设置为失败，获取锁流程结束。这种设计会极大降低系统的并发度，并不满足我们实际的需求。所以就需要下面这种流程，也就是AQS框架的处理流程。 (2) 存在某种排队等候机制，线程继续等待，仍然保留获取锁的可能，获取锁流程仍在继续。 对于问题1的第二种情况，既然说到了排队等候机制，那么就一定会有某种队列形成，这样的队列是什么数据结构呢？ 处于排队等候机制中的线程，什么时候可以有机会获取锁呢？ 如果处于排队等候机制中的线程一直无法获取锁，还是需要一直等待吗，还是有别的策略来解决这一问题？ 带着非公平锁的这些问题，再看下公平锁源码中获锁的方式： 123456789// java.util.concurrent.locks.ReentrantLock#FairSyncstatic final class FairSync extends Sync &#123; ... final void lock() &#123; acquire(1); &#125; ...&#125; 看到这块代码，我们可能会存在这种疑问：Lock函数通过Acquire方法进行加锁，但是具体是如何加锁的呢？ 结合公平锁和非公平锁的加锁流程，虽然流程上有一定的不同，但是都调用了Acquire方法，而Acquire方法是FairSync和UnfairSync的父类AQS中的核心方法。 对于上边提到的问题，其实在ReentrantLock类源码中都无法解答，而这些问题的答案，都是位于Acquire方法所在的类AbstractQueuedSynchronizer中，也就是本文的核心——AQS。 2 AQS首先，我们通过下面的架构图来整体了解一下AQS框架： 上图中有颜色的为Method，无颜色的为Attribution。 总的来说，AQS框架共分为五层，自上而下由浅入深，从AQS对外暴露的API到底层基础数据。 当有自定义同步器接入时，只需重写第一层所需要的部分方法即可，不需要关注底层具体的实现流程。当自定义同步器进行加锁或者解锁操作时，先经过第一层的API进入AQS内部方法，然后经过第二层进行锁的获取，接着对于获取锁失败的流程，进入第三层和第四层的等待队列处理，而这些处理方式均依赖于第五层的基础数据提供层。 下面我们会从整体到细节，从流程到方法逐一剖析AQS框架，主要分析过程如下： 2.1 原理概览AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。 主要原理图如下： AQS使用一个Volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。 2.1.1 AQS数据结构先来看下AQS中最基本的数据结构——Node，Node即为上面CLH变体队列中的节点。 解释一下几个方法和属性值的含义： 线程两种锁的模式： waitStatus有下面几个枚举值： 2.1.2 同步状态State在了解数据结构后，接下来了解一下AQS的同步状态——State。AQS中维护了一个名为state的字段，意为同步状态，是由Volatile修饰的，用于展示当前临界资源的获锁情况。 123// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate volatile int state; 下面提供了几个访问这个字段的方法： 这几个方法都是Final修饰的，说明子类中无法重写它们。我们可以通过修改State字段表示的同步状态来实现多线程的独占模式和共享模式（加锁过程）。 对于我们自定义的同步工具，需要自定义获取同步状态和释放状态的方式，也就是AQS架构图中的第一层：API层。 2.2 AQS重要方法与ReentrantLock的关联从架构图中可以得知，AQS提供了大量用于自定义同步器实现的Protected方法。自定义同步器实现的相关方法也只是为了通过修改State字段来实现多线程的独占模式或者共享模式。自定义同步器需要实现以下方法（ReentrantLock需要实现的方法如下，并不是全部）： 一般来说，自定义同步器要么是独占方式，要么是共享方式，它们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。ReentrantLock是独占锁，所以实现了tryAcquire-tryRelease。 以非公平锁为例，这里主要阐述一下非公平锁与AQS之间方法的关联之处，具体每一处核心方法的作用会在文章后面详细进行阐述。 为了帮助大家理解ReentrantLock和AQS之间方法的交互过程，以非公平锁为例，我们将加锁和解锁的交互流程单独拎出来强调一下，以便于对后续内容的理解。 加锁： 通过ReentrantLock的加锁方法Lock进行加锁操作。 会调用到内部类Sync的Lock方法，由于Sync#lock是抽象方法，根据ReentrantLock初始化选择的公平锁和非公平锁，执行相关内部类的Lock方法，本质上都会执行AQS的Acquire方法。 AQS的Acquire方法会执行tryAcquire方法，但是由于tryAcquire需要自定义同步器实现，因此执行了ReentrantLock中的tryAcquire方法，由于ReentrantLock是通过公平锁和非公平锁内部类实现的tryAcquire方法，因此会根据锁类型不同，执行不同的tryAcquire。 tryAcquire是获取锁逻辑，获取失败后，会执行框架AQS的后续逻辑，跟ReentrantLock自定义同步器无关。解锁： 通过ReentrantLock的解锁方法Unlock进行解锁。 Unlock会调用内部类Sync的Release方法，该方法继承于AQS。 Release中会调用tryRelease方法，tryRelease需要自定义同步器实现，tryRelease只在ReentrantLock中的Sync实现，因此可以看出，释放锁的过程，并不区分是否为公平锁。 释放成功后，所有处理由AQS框架完成，与自定义同步器无关。 通过上面的描述，大概可以总结出ReentrantLock加锁解锁时API层核心方法的映射关系。 2.3 通过ReentrantLock理解AQSReentrantLock中公平锁和非公平锁在底层是相同的，这里以非公平锁为例进行分析。 在非公平锁中，有一段这样的代码： 123456789101112// java.util.concurrent.locks.ReentrantLockstatic final class NonfairSync extends Sync &#123; ... final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; ...&#125; 看一下这个Acquire是怎么写的： 123456// java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 再看一下tryAcquire方法： 12345// java.util.concurrent.locks.AbstractQueuedSynchronizerprotected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; 可以看出，这里只是AQS的简单实现，具体获取锁的实现方法是由各自的公平锁和非公平锁单独实现的（以ReentrantLock为例）。如果该方法返回了True，则说明当前线程获取锁成功，就不用往后执行了；如果获取失败，就需要加入到等待队列中。下面会详细解释线程是何时以及怎样被加入进等待队列中的。 2.3.1 线程加入等待队列2.3.1.1 加入队列的时机当执行Acquire(1)时，会通过tryAcquire获取锁。在这种情况下，如果获取锁失败，就会调用addWaiter加入到等待队列中去。 2.3.1.2 如何加入队列获取锁失败后，会执行addWaiter(Node.EXCLUSIVE)加入等待队列，具体实现方法如下： 12345678910111213141516171819// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125;private final boolean compareAndSetTail(Node expect, Node update) &#123; return unsafe.compareAndSwapObject(this, tailOffset, expect, update);&#125; 主要的流程如下： 通过当前的线程和锁模式新建一个节点。 Pred指针指向尾节点Tail。 将New中Node的Prev指针指向Pred。 通过compareAndSetTail方法，完成尾节点的设置。这个方法主要是对tailOffset和Expect进行比较，如果tailOffset的Node和Expect的Node地址是相同的，那么设置Tail的值为Update的值。 12345678910111213// java.util.concurrent.locks.AbstractQueuedSynchronizerstatic &#123; try &#123; stateOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;state&quot;)); headOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;head&quot;)); tailOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;tail&quot;)); waitStatusOffset = unsafe.objectFieldOffset(Node.class.getDeclaredField(&quot;waitStatus&quot;)); nextOffset = unsafe.objectFieldOffset(Node.class.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125; 从AQS的静态代码块可以看出，都是获取一个对象的属性相对于该对象在内存当中的偏移量，这样我们就可以根据这个偏移量在对象内存当中找到这个属性。tailOffset指的是tail对应的偏移量，所以这个时候会将new出来的Node置为当前队列的尾节点。同时，由于是双向链表，也需要将前一个节点指向尾节点。 如果Pred指针是Null（说明等待队列中没有元素），或者当前Pred指针和Tail指向的位置不同（说明被别的线程已经修改），就需要看一下Enq的方法。 1234567891011121314151617 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 如果没有被初始化，需要进行初始化一个头结点出来。但请注意，初始化的头结点并不是当前线程节点，而是调用了无参构造函数的节点。如果经历了初始化或者并发导致队列中有元素，则与之前的方法相同。其实，addWaiter就是一个在双端链表添加尾节点的操作，需要注意的是，双端链表的头结点是一个无参构造函数的头结点。 总结一下，线程获取锁的时候，过程大体如下： 当没有线程获取到锁时，线程1获取锁成功。 线程2申请锁，但是锁被线程1占有。 如果再有线程要获取锁，依次在队列中往后排队即可。回到上边的代码，hasQueuedPredecessors是公平锁加锁时判断等待队列中是否存在有效节点的方法。如果返回False，说明当前线程可以争取共享资源；如果返回True，说明队列中存在有效节点，当前线程必须加入到等待队列中。 1234567891011// java.util.concurrent.locks.ReentrantLockpublic final boolean hasQueuedPredecessors() &#123; // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 看到这里，我们理解一下h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());为什么要判断的头结点的下一个节点？第一个节点储存的数据是什么？ 双向链表中，第一个节点为虚节点，其实并不存储任何信息，只是占位。真正的第一个有数据的节点，是在第二个节点开始的。当h != t时： 如果(s = h.next) == null，等待队列正在有线程进行初始化，但只是进行到了Tail指向Head，没有将Head指向Tail，此时队列中有元素，需要返回True（这块具体见下边代码分析）。 如果(s = h.next) != null，说明此时队列中至少有一个有效节点。如果此时s.thread == Thread.currentThread()，说明等待队列的第一个有效节点中的线程与当前线程相同，那么当前线程是可以获取资源的；如果s.thread != Thread.currentThread()，说明等待队列的第一个有效节点线程与当前线程不同，当前线程必须加入进等待队列。 123456789101112// java.util.concurrent.locks.AbstractQueuedSynchronizer#enqif (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head;&#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125;&#125; 节点入队不是原子操作，所以会出现短暂的head != tail，此时Tail指向最后一个节点，而且Tail指向Head。如果Head没有指向Tail（可见5、6、7行），这种情况下也需要将相关线程加入队列中。所以这块代码是为了解决极端情况下的并发问题。 2.3.1.3 等待队列中线程出队列时机回到最初的源码： 123456// java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 上文解释了addWaiter方法，这个方法其实就是把对应的线程以Node的数据结构形式加入到双端队列里，返回的是一个包含该线程的Node。而这个Node会作为参数，进入到acquireQueued方法中。acquireQueued方法可以对排队中的线程进行“获锁”操作。 总的来说，一个线程获取锁失败了，被放入等待队列，acquireQueued会把放入队列中的线程不断去获取锁，直到获取成功或者不再需要获取（中断）。 下面我们从“何时出队列？”和“如何出队列？”两个方向来分析一下acquireQueued源码： 1234567891011121314151617181920212223242526272829// java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; // 标记是否成功拿到资源 boolean failed = true; try &#123; // 标记等待过程中是否中断过 boolean interrupted = false; // 开始自旋，要么获取锁，要么中断 for (;;) &#123; // 获取当前节点的前驱节点 final Node p = node.predecessor(); // 如果p是头结点，说明当前节点在真实数据队列的首部，就尝试获取锁（别忘了头结点是虚节点） if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取锁成功，头指针移动到当前node setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 说明p为头节点且当前没有获取到锁（可能是非公平锁被抢占了）或者是p不为头结点，这个时候就要判断当前node是否要被阻塞（被阻塞条件：前驱节点的waitStatus为-1），防止无限循环浪费资源。具体两个方法下面细细分析 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 注：setHead方法是把当前节点置为虚节点，但并没有修改waitStatus，因为它是一直需要用的数据。 123456789101112131415161718192021222324252627282930// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void setHead(Node node) &#123; head = node; node.thread = null; node.prev = null;&#125;// java.util.concurrent.locks.AbstractQueuedSynchronizer// 靠前驱节点判断当前线程是否应该被阻塞private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 获取头结点的节点状态 int ws = pred.waitStatus; // 说明头结点处于唤醒状态 if (ws == Node.SIGNAL) return true; // 通过枚举值我们知道waitStatus&gt;0是取消状态 if (ws &gt; 0) &#123; do &#123; // 循环向前查找取消节点，把取消节点从队列中剔除 node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 设置前任节点等待状态为SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt主要用于挂起当前线程，阻塞调用栈，返回当前线程的中断状态。 123456// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 上述方法的流程图如下： 从上图可以看出，跳出当前循环的条件是当“前置节点是头结点，且当前线程获取锁成功”。为了防止因死循环导致CPU资源被浪费，我们会判断前置节点的状态来决定是否要将当前线程挂起，具体挂起流程用流程图表示如下（shouldParkAfterFailedAcquire流程）： 从队列中释放节点的疑虑打消了，那么又有新问题了： shouldParkAfterFailedAcquire中取消节点是怎么生成的呢？什么时候会把一个节点的waitStatus设置为-1？ 是在什么时间释放节点通知到被挂起的线程呢？ 2.3.2 CANCELLED状态节点生成acquireQueued方法中的Finally代码： 12345678910111213141516171819// java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; ... for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; ... failed = false; ... &#125; ... &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 通过cancelAcquire方法，将Node的状态标记为CANCELLED。接下来，我们逐行来分析这个方法的原理： 123456789101112131415161718192021222324252627282930313233343536// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void cancelAcquire(Node node) &#123; // 将无效节点过滤 if (node == null) return; // 设置该节点不关联任何线程，也就是虚节点 node.thread = null; Node pred = node.prev; // 通过前驱节点，跳过取消状态的node while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取过滤后的前驱节点的后继节点 Node predNext = pred.next; // 把当前node的状态设置为CANCELLED node.waitStatus = Node.CANCELLED; // 如果当前节点是尾节点，将从后往前的第一个非取消状态的节点设置为尾节点 // 更新失败的话，则进入else，如果更新成功，将tail的后继节点设置为null if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; int ws; // 如果当前节点不是head的后继节点，1:判断当前节点前驱节点的是否为SIGNAL，2:如果不是，则把前驱节点设置为SINGAL看是否成功 // 如果1和2中有一个为true，再判断当前节点的线程是否为null // 如果上述条件都满足，把当前节点的前驱节点的后继指针指向当前节点的后继节点 if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果当前节点是head的后继节点，或者上述条件不满足，那就唤醒当前节点的后继节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 当前的流程： 获取当前节点的前驱节点，如果前驱节点的状态是CANCELLED，那就一直往前遍历，找到第一个waitStatus &lt;= 0的节点，将找到的Pred节点和当前Node关联，将当前Node设置为CANCELLED。 根据当前节点的位置，考虑以下三种情况： (1) 当前节点是尾节点。 (2) 当前节点是Head的后继节点。 (3) 当前节点不是Head的后继节点，也不是尾节点。 根据上述第二条，我们来分析每一种情况的流程。 当前节点是尾节点。 当前节点是Head的后继节点。 当前节点不是Head的后继节点，也不是尾节点。 通过上面的流程，我们对于CANCELLED节点状态的产生和变化已经有了大致的了解，但是为什么所有的变化都是对Next指针进行了操作，而没有对Prev指针进行操作呢？什么情况下会对Prev指针进行操作？ 执行cancelAcquire的时候，当前节点的前置节点可能已经从队列中出去了（已经执行过Try代码块中的shouldParkAfterFailedAcquire方法了），如果此时修改Prev指针，有可能会导致Prev指向另一个已经移除队列的Node，因此这块变化Prev指针不安全。 shouldParkAfterFailedAcquire方法中，会执行下面的代码，其实就是在处理Prev指针。shouldParkAfterFailedAcquire是获取锁失败的情况下才会执行，进入该方法后，说明共享资源已被获取，当前节点之前的节点都不会出现变化，因此这个时候变更Prev指针比较安全。 123do &#123; node.prev = pred = pred.prev;&#125; while (pred.waitStatus &gt; 0); 2.3.3 如何解锁我们已经剖析了加锁过程中的基本流程，接下来再对解锁的基本流程进行分析。由于ReentrantLock在解锁的时候，并不区分公平锁和非公平锁，所以我们直接看解锁的源码： 12345// java.util.concurrent.locks.ReentrantLockpublic void unlock() &#123; sync.release(1);&#125; 可以看到，本质释放锁的地方，是通过框架来完成的。 123456789101112// java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 在ReentrantLock里面的公平锁和非公平锁的父类Sync定义了可重入锁的释放锁机制。 123456789101112131415161718// java.util.concurrent.locks.ReentrantLock.Sync// 方法返回当前锁是不是没有被线程持有protected final boolean tryRelease(int releases) &#123; // 减少可重入次数 int c = getState() - releases; // 当前线程不是持有锁的线程，抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 如果持有线程全部释放，将当前独占锁所有线程设置为null，并更新state if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 我们来解释下述源码： 1234567891011121314// java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final boolean release(int arg) &#123; // 上边自定义的tryRelease如果返回true，说明该锁没有被任何线程持有 if (tryRelease(arg)) &#123; // 获取头结点 Node h = head; // 头结点不为空并且头结点的waitStatus不是初始化节点情况，解除线程挂起状态 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 这里的判断条件为什么是h != null &amp;&amp; h.waitStatus != 0？ h == null Head还没初始化。初始情况下，head == null，第一个节点入队，Head会被初始化一个虚拟节点。所以说，这里如果还没来得及入队，就会出现head == null 的情况。 h != null &amp;&amp; waitStatus == 0 表明后继节点对应的线程仍在运行中，不需要唤醒。 h != null &amp;&amp; waitStatus &lt; 0 表明后继节点可能被阻塞了，需要唤醒。 再看一下unparkSuccessor方法： 123456789101112131415161718192021// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void unparkSuccessor(Node node) &#123; // 获取头结点waitStatus int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); // 获取当前节点的下一个节点 Node s = node.next; // 如果下个节点是null或者下个节点被cancelled，就找到队列最开始的非cancelled的节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 就从尾部节点开始找，到队首，找到队列第一个waitStatus&lt;0的节点。 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; // 如果当前节点的下个节点不为空，而且状态&lt;=0，就把当前节点unpark if (s != null) LockSupport.unpark(s.thread);&#125; 为什么要从后往前找第一个非Cancelled的节点呢？原因如下。 之前的addWaiter方法： 12345678910111213141516// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 我们从这里可以看到，节点入队并不是原子操作，也就是说，node.prev = pred; compareAndSetTail(pred, node) 这两个地方可以看作Tail入队的原子操作，但是此时pred.next = node;还没执行，如果这个时候执行了unparkSuccessor方法，就没办法从前往后找了，所以需要从后往前找。还有一点原因，在产生CANCELLED状态节点的时候，先断开的是Next指针，Prev指针并未断开，因此也是必须要从后往前遍历才能够遍历完全部的Node。 综上所述，如果是从前往后找，由于极端情况下入队的非原子操作和CANCELLED节点产生过程中断开Next指针的操作，可能会导致无法遍历所有的节点。所以，唤醒对应的线程后，对应的线程就会继续往下执行。继续执行acquireQueued方法以后，中断如何处理？ 2.3.4 中断恢复后的执行流程唤醒后，会执行return Thread.interrupted();，这个函数返回的是当前执行线程的中断状态，并清除。 123456// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 再回到acquireQueued代码，当parkAndCheckInterrupt返回True或者False的时候，interrupted的值不同，但都会执行下次循环。如果这个时候获取锁成功，就会把当前interrupted返回。 1234567891011121314151617181920212223// java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 如果acquireQueued为True，就会执行selfInterrupt方法。 12345// java.util.concurrent.locks.AbstractQueuedSynchronizerstatic void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; 该方法其实是为了中断线程。但为什么获取了锁以后还要中断线程呢？这部分属于Java提供的协作式中断知识内容，感兴趣同学可以查阅一下。这里简单介绍一下： 当中断线程被唤醒时，并不知道被唤醒的原因，可能是当前线程在等待中被中断，也可能是释放了锁以后被唤醒。因此我们通过Thread.interrupted()方法检查中断标记（该方法返回了当前线程的中断状态，并将当前线程的中断标识设置为False），并记录下来，如果发现该线程被中断过，就再中断一次。 线程在等待资源的过程中被唤醒，唤醒后还是会不断地去尝试获取锁，直到抢到锁为止。也就是说，在整个流程中，并不响应中断，只是记录中断记录。最后抢到锁返回了，那么如果被中断过的话，就需要补充一次中断。 这里的处理方式主要是运用线程池中基本运作单元Worder中的runWorker，通过Thread.interrupted()进行额外的判断处理，感兴趣的同学可以看下ThreadPoolExecutor源码。 2.3.5 小结Q：某个线程获取锁失败的后续流程是什么呢？A：存在某种排队等候机制，线程继续等待，仍然保留获取锁的可能，获取锁流程仍在继续。 Q：既然说到了排队等候机制，那么就一定会有某种队列形成，这样的队列是什么数据结构呢？A：是CLH变体的FIFO双端队列。 Q：处于排队等候机制中的线程，什么时候可以有机会获取锁呢？A：可以详细看下2.3.1.3小节。 Q：如果处于排队等候机制中的线程一直无法获取锁，需要一直等待么？还是有别的策略来解决这一问题？A：线程所在节点的状态会变成取消状态，取消状态的节点会从队列中释放，具体可见2.3.2小节。 Q：Lock函数通过Acquire方法进行加锁，但是具体是如何加锁的呢？A：AQS的Acquire会调用tryAcquire方法，tryAcquire由各个自定义同步器实现，通过tryAcquire完成加锁过程。 3 AQS应用3.1 ReentrantLock的可重入应用ReentrantLock的可重入性是AQS很好的应用之一，在了解完上述知识点以后，我们很容易得知ReentrantLock实现可重入的方法。在ReentrantLock里面，不管是公平锁还是非公平锁，都有一段逻辑。 公平锁： 123456789101112131415// java.util.concurrent.locks.ReentrantLock.FairSync#tryAcquireif (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125;&#125;else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true;&#125; 非公平锁： 123456789101112131415// java.util.concurrent.locks.ReentrantLock.Sync#nonfairTryAcquireif (c == 0) &#123; if (compareAndSetState(0, acquires))&#123; setExclusiveOwnerThread(current); return true; &#125;&#125;else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true;&#125; 从上面这两段都可以看到，有一个同步状态State来控制整体可重入的情况。State是Volatile修饰的，用于保证一定的可见性和有序性。 123// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate volatile int state; 接下来看State这个字段主要的过程： State初始化的时候为0，表示没有任何线程持有锁。 当有线程持有该锁时，值就会在原来的基础上+1，同一个线程多次获得锁是，就会多次+1，这里就是可重入的概念。 解锁也是对这个字段-1，一直到0，此线程对锁释放。 3.2 JUC中的应用场景除了上边ReentrantLock的可重入性的应用，AQS作为并发编程的框架，为很多其他同步工具提供了良好的解决方案。下面列出了JUC中的几种同步工具，大体介绍一下AQS的应用场景： 3.3 自定义同步工具了解AQS基本原理以后，按照上面所说的AQS知识点，自己实现一个同步工具。 123456789101112131415161718192021222324252627282930public class LeeLock &#123; private static class Sync extends AbstractQueuedSynchronizer &#123; @Override protected boolean tryAcquire (int arg) &#123; return compareAndSetState(0, 1); &#125; @Override protected boolean tryRelease (int arg) &#123; setState(0); return true; &#125; @Override protected boolean isHeldExclusively () &#123; return getState() == 1; &#125; &#125; private Sync sync = new Sync(); public void lock () &#123; sync.acquire(1); &#125; public void unlock () &#123; sync.release(1); &#125;&#125; 通过我们自己定义的Lock完成一定的同步功能。 1234567891011121314151617181920212223242526272829303132public class LeeMain &#123; static int count = 0; static LeeLock leeLock = new LeeLock(); public static void main (String[] args) throws InterruptedException &#123; Runnable runnable = new Runnable() &#123; @Override public void run () &#123; try &#123; leeLock.lock(); for (int i = 0; i &lt; 10000; i++) &#123; count++; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; leeLock.unlock(); &#125; &#125; &#125;; Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(count); &#125;&#125; 上述代码每次运行结果都会是20000。通过简单的几行代码就能实现同步功能，这就是AQS的强大之处。 本文章转载自： https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html","categories":[],"tags":[]},{"title":"vagrant创建管理虚拟机","slug":"yq/hn553n","date":"2019-07-13T12:46:25.000Z","updated":"2021-05-07T03:02:08.219Z","comments":true,"path":"2019/07/13/yq/hn553n/","link":"","permalink":"https://voox.cc/2019/07/13/yq/hn553n/","excerpt":"","text":"安装前准备 Install the latest version of Vagrant. Install VirtualBox vargrant 使用在任意目录下创建 vagrantfile 文件 123456789101112131415161718192021222324252627Vagrant.configure(&quot;2&quot;) do |config| (1..3).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;private_network&quot;, ip: &quot;192.168.56.#&#123;99+i&#125;&quot;, netmask: &quot;255.255.255.0&quot; # 设置主机与虚拟机的共享目录 # node.vm.synced_folder &quot;~/Documents/vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 4 end end endend 在文件所在的目录下，执行vagrant up 创建并启动虚拟机， 执行vagrant ssh 连接进入到虚拟机控制台。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://voox.cc/categories/Linux/"}],"tags":[{"name":"vagrant","slug":"vagrant","permalink":"https://voox.cc/tags/vagrant/"}]},{"title":"junit5 annotation","slug":"yq/ivmag1","date":"2019-04-10T03:00:25.000Z","updated":"2021-05-07T03:02:08.211Z","comments":true,"path":"2019/04/10/yq/ivmag1/","link":"","permalink":"https://voox.cc/2019/04/10/yq/ivmag1/","excerpt":"","text":"junit5 annotation 详细 Annotation Description @Test Denotes that a method is a test method. Unlike JUnit 4’s @Test annotation, this annotation does not declare any attributes, since test extensions in JUnit Jupiter operate based on their own dedicated annotations. Such methods are inherited unless they are overridden. @ParameterizedTest Denotes that a method is a parameterized test. Such methods are inherited unless they are overridden. @RepeatedTest Denotes that a method is a test template for a repeated test. Such methods are inherited unless they are overridden. @TestFactory Denotes that a method is a test factory for dynamic tests. Such methods are inherited unless they are overridden. @TestTemplate Denotes that a method is a template for test cases designed to be invoked multiple times depending on the number of invocation contexts returned by the registered providers. Such methods are inherited unless they are overridden. @TestMethodOrder Used to configure the test method execution order for the annotated test class; similar to JUnit 4’s @FixMethodOrder. Such annotations are inherited. @TestInstance Used to configure the test instance lifecycle for the annotated test class. Such annotations are inherited. @DisplayName Declares a custom display name for the test class or test method. Such annotations are not inherited. @DisplayNameGeneration Declares a custom display name generator for the test class. Such annotations are inherited. @BeforeEach Denotes that the annotated method should be executed before each @Test, @RepeatedTest, @ParameterizedTest, or @TestFactory method in the current class; analogous to JUnit 4’s @Before. Such methods are inherited unless they are overridden. @AfterEach Denotes that the annotated method should be executed after each @Test, @RepeatedTest, @ParameterizedTest, or @TestFactory method in the current class; analogous to JUnit 4’s @After. Such methods are inherited unless they are overridden. @BeforeAll Denotes that the annotated method should be executed before all @Test, @RepeatedTest, @ParameterizedTest, and @TestFactory methods in the current class; analogous to JUnit 4’s @BeforeClass. Such methods are inherited (unless they are hidden or overridden) and must be static (unless the “per-class” test instance lifecycle is used). @AfterAll Denotes that the annotated method should be executed after all @Test, @RepeatedTest, @ParameterizedTest, and @TestFactory methods in the current class; analogous to JUnit 4’s @AfterClass. Such methods are inherited (unless they are hidden or overridden) and must be static (unless the “per-class” test instance lifecycle is used). @Nested Denotes that the annotated class is a non-static nested test class. @BeforeAll and @AfterAll methods cannot be used directly in a @Nested test class unless the “per-class” test instance lifecycle is used. Such annotations are not inherited. @Tag Used to declare tags for filtering tests, either at the class or method level; analogous to test groups in TestNG or Categories in JUnit 4. Such annotations are inherited at the class level but not at the method level. @Disabled Used to disable a test class or test method; analogous to JUnit 4’s @Ignore. Such annotations are not inherited. @Timeout Used to fail a test, test factory, test template, or lifecycle method if its execution exceeds a given duration. Such annotations are inherited. @ExtendWith Used to register extensions declaratively. Such annotations are inherited. @RegisterExtension Used to register extensions programmatically via fields. Such fields are inherited unless they are shadowed. @TempDir Used to supply a temporary directory via field injection or parameter injection in a lifecycle method or test method; located in the org.junit.jupiter.api.io package.","categories":[{"name":"Java","slug":"Java","permalink":"https://voox.cc/categories/Java/"}],"tags":[{"name":"junit","slug":"junit","permalink":"https://voox.cc/tags/junit/"}]},{"title":"update by partition","slug":"update-by-partition","date":"2019-03-01T07:25:19.000Z","updated":"2021-05-07T03:01:42.213Z","comments":true,"path":"2019/03/01/update-by-partition/","link":"","permalink":"https://voox.cc/2019/03/01/update-by-partition/","excerpt":"","text":"sql update top 5 123456789SELECT ROW_NUMBER() OVER (PARTITION BY col1, col2 ORDER BY x desc) AS r, t.id FROM some_table t where condition = &#x27;xxx&#x27;) Awhere A.r &lt; 5;","categories":[],"tags":[{"name":"sql","slug":"sql","permalink":"https://voox.cc/tags/sql/"},{"name":"postgresql","slug":"postgresql","permalink":"https://voox.cc/tags/postgresql/"}]},{"title":"Docker中exec和attach区别","slug":"Docker中exec和attach区别","date":"2018-08-08T02:57:20.000Z","updated":"2021-05-07T03:01:42.177Z","comments":true,"path":"2018/08/08/Docker中exec和attach区别/","link":"","permalink":"https://voox.cc/2018/08/08/Docker%E4%B8%ADexec%E5%92%8Cattach%E5%8C%BA%E5%88%AB/","excerpt":"","text":"docker attach 执行后会进入到 container 中docker exec 执行后，命令执行返回值并显示到宿主机中。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"}]},{"title":"Docker alias","slug":"docker-alias","date":"2018-02-11T06:07:03.000Z","updated":"2021-05-07T03:01:42.197Z","comments":true,"path":"2018/02/11/docker-alias/","link":"","permalink":"https://voox.cc/2018/02/11/docker-alias/","excerpt":"","text":"Docker alias and function1234567891011121314151617181920212223242526272829303132333435363738394041424344Get latest container IDalias dl=&quot;docker ps -l -q&quot;Get container processalias dps=&quot;docker ps&quot;Get process included stop containeralias dpa=&quot;docker ps -a&quot;Get imagesalias di=&quot;docker images&quot;Get container IPalias dip=&quot;docker inspect --format &#x27;&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;&#x27;&quot;Run deamonized container, e.g., $dkd base /bin/echo helloalias dkd=&quot;docker run -d -P&quot;Run interactive container, e.g., $dki base /bin/bashalias dki=&quot;docker run -i -t -P&quot;Execute interactive container, e.g., $dex base /bin/bashalias dex=&quot;docker exec -i -t&quot;Stop all containersdstop() &#123; docker stop $(docker ps -a -q); &#125;Remove all containersdrm() &#123; docker rm $(docker ps -a -q); &#125;Stop and Remove all containersalias drmf=&#x27;docker stop $(docker ps -a -q) &amp;&amp; docker rm $(docker ps -a -q)&#x27;Remove all imagesdri() &#123; docker rmi $(docker images -q); &#125;Dockerfile build, e.g., $dbu tcnksm/testdbu() &#123; docker build -t=$1 .; &#125;Show all alias related dockerdalias() &#123; alias | grep &#x27;docker&#x27; | sed &quot;s/^([^=])=(.)/\\1 =&gt; \\2/&quot;| sed &quot;s/[&#x27;|&#x27;]//g&quot; | sort; &#125;Bash into running containerdbash() &#123; docker exec -it $(docker ps -aqf &quot;name=$1&quot;) bash; &#125;","categories":[],"tags":[{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]},{"title":"Docker常用命令及使用","slug":"Docker常用命令及使用","date":"2017-11-20T05:45:18.000Z","updated":"2021-05-07T03:01:42.177Z","comments":true,"path":"2017/11/20/Docker常用命令及使用/","link":"","permalink":"https://voox.cc/2017/11/20/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E4%BD%BF%E7%94%A8/","excerpt":"","text":"以下是在 centos 系统下执行 123456789101112131415161718192021sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.reposudo yum makecache fastsudo yum install docker-cevi /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot; : [ &quot;https://xxxx.mirror.aliyuncs.com&quot; ], &quot;insecure-registries&quot; : [ &quot;registry.mirrors.aliyuncs.com&quot; ], &quot;debug&quot; : true, &quot;experimental&quot; : true&#125;sudo systemctl start docker","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]}],"categories":[{"name":"netty","slug":"netty","permalink":"https://voox.cc/categories/netty/"},{"name":"nio","slug":"netty/nio","permalink":"https://voox.cc/categories/netty/nio/"},{"name":"navicat","slug":"navicat","permalink":"https://voox.cc/categories/navicat/"},{"name":"bios","slug":"bios","permalink":"https://voox.cc/categories/bios/"},{"name":"Linux","slug":"Linux","permalink":"https://voox.cc/categories/Linux/"},{"name":"Java","slug":"Java","permalink":"https://voox.cc/categories/Java/"}],"tags":[{"name":"netty","slug":"netty","permalink":"https://voox.cc/tags/netty/"},{"name":"nio","slug":"nio","permalink":"https://voox.cc/tags/nio/"},{"name":"navicat","slug":"navicat","permalink":"https://voox.cc/tags/navicat/"},{"name":"bios","slug":"bios","permalink":"https://voox.cc/tags/bios/"},{"name":"java","slug":"java","permalink":"https://voox.cc/tags/java/"},{"name":"spring","slug":"spring","permalink":"https://voox.cc/tags/spring/"},{"name":"linux","slug":"linux","permalink":"https://voox.cc/tags/linux/"},{"name":"io","slug":"io","permalink":"https://voox.cc/tags/io/"},{"name":"karnel","slug":"karnel","permalink":"https://voox.cc/tags/karnel/"},{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"https://voox.cc/tags/k8s/"},{"name":"k8s-cluster","slug":"k8s-cluster","permalink":"https://voox.cc/tags/k8s-cluster/"},{"name":"rpc","slug":"rpc","permalink":"https://voox.cc/tags/rpc/"},{"name":"vagrant","slug":"vagrant","permalink":"https://voox.cc/tags/vagrant/"},{"name":"junit","slug":"junit","permalink":"https://voox.cc/tags/junit/"},{"name":"sql","slug":"sql","permalink":"https://voox.cc/tags/sql/"},{"name":"postgresql","slug":"postgresql","permalink":"https://voox.cc/tags/postgresql/"},{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]}