{"meta":{"title":"TJ's Blog","subtitle":null,"description":"backend;java;typescript;graphql;fullstack","author":"TJ","url":"https://voox.cc","root":"/"},"pages":[{"title":"","date":"2020-10-12T14:45:36.422Z","updated":"2020-10-12T14:45:36.422Z","comments":true,"path":"manifest.json","permalink":"https://voox.cc/manifest.json","excerpt":"","text":"{\"short_name\":\"TJ\",\"name\":\"TJ\",\"lang\":\"en\",\"description\":\"TJ's Blog\",\"start_url\":\"https://voox.cc\",\"background_color\":\"#00aae7\",\"theme_color\":\"#00aae7\",\"dir\":\"ltr\",\"display\":\"standalone\",\"orientation\":\"any\",\"prefer_related_applications\":\"false\"}"},{"title":"categories","date":"2019-01-31T02:54:24.000Z","updated":"2019-01-31T02:54:24.806Z","comments":true,"path":"categories/index.html","permalink":"https://voox.cc/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-19T06:59:53.000Z","updated":"2020-10-09T03:17:22.556Z","comments":false,"path":"about/index.html","permalink":"https://voox.cc/about/index.html","excerpt":"","text":"About Me个人信息 TJ / 男背景 \b 及其技能 2011 年至今 Java 企业级开发经验 熟悉基于 Spring-cloud 微服务架构 熟悉 Nodejs, Docker, RabbitMQ, Redis, Postgres, Typescript, GraphQL 熟悉 Github, Bitbucket 参与过开源项目和开源社区的建设和开发 联系方式微信 Email (base64) aUB2b294LmNj Github https://github.com/aooppo"},{"title":"about","date":"2020-03-19T06:59:53.000Z","updated":"2020-10-08T14:49:57.033Z","comments":false,"path":"projects/index.html","permalink":"https://voox.cc/projects/index.html","excerpt":"","text":""}],"posts":[{"title":"K8s Devops环境搭建准备","slug":"K8s部署环境准备","date":"2020-10-12T09:26:35.000Z","updated":"2020-10-12T12:29:52.817Z","comments":true,"path":"2020/10/12/K8s部署环境准备/","link":"","permalink":"https://voox.cc/2020/10/12/K8s%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"1.安装依赖 vagrant 和virtual box创建文件 vagrantfile以下是文件内容： Vagrant.configure(&quot;2&quot;) do |config| (1..3).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;private_network&quot;, ip: &quot;192.168.56.#&#123;99+i&#125;&quot;, netmask: &quot;255.255.255.0&quot; # 设置主机与虚拟机的共享目录 # node.vm.synced_folder &quot;~/Documents/vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 4 end end endend 然后执行 vagrant up 设置每一个虚拟机的网络环境 连接方式需要变成NAT网络每一个虚拟机的mac 地址需要刷新重新生成 # 检查eth0 ipip route show# 确认eth0 对应的ip 在每个虚拟机不一样ip addr# 测试能否联通其他虚拟机ping 10.0.2.5#测试能否联通外网ping baidu.com 进入每一台虚拟机 开启密码访问登录 vagrant ssh k8s-node1su rootvi /etc/ssh/sshd_config# 输入i 进入编辑模式# 更改 PasswordAuthentication no 设成yes# 方便远程可以通过用户密码的方式登录PasswordAuthentication yes#编辑后按ESC 输入wq 保存并退出# 重启sshd 服务 让其生效service sshd restart 通过ssh 登录各个虚拟机 # 关闭防火墙systemctl stop firewalld# 禁用防火墙systemctl disable firewalld#关闭linux安全策略sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/configsetenforce 0 #（1）临时关闭swap分区, 重启失效;swapoff -a#（2）永久关闭swap分区sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab 添加主机名与ip对应关系 vi /etc/hosts10.0.2.5 k8s-node110.0.2.4 k8s-node210.0.2.15 k8s-node3# 如果主机名没设置对hostnamectl set-hostname &lt;newhostname&gt; # 桥接的ipv4 流量传递到iptablescat &gt;/etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables =1net.bridge.bridge-nf-call-iptables =1EOFsysctl --system 2. k8s-集群搭建-安装Docker、kubelet、kubeadm、kubectl# 卸载原来的dockersudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine# 安装依赖sudo yum update -y &amp;&amp; sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # 添加官方yum库sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装dockersudo yum install docker-ce docker-ce-cli containerd.io# 查看docker版本docker --version# 开机启动systemctl enable --now docker 添加docker加速 sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://yourcode.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker #更换yum 源为aliyuncat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3systemctl enable kubeletsystemctl start kubelet 3. k8s master节点安装kubeadm init \\ --apiserver-advertise-address 10.0.2.5 \\ --cert-dir /etc/kubernetes/pki \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version 1.17.3 \\ --pod-network-cidr 10.244.0.0/16 \\ --service-cidr 10.96.0.0/16 \\ 在master节点执行文件， 执行文件前设置文件可执行(700) #!/bin/bashimages=( kube-apiserver:v1.17.3 kube-proxy:v1.17.3 kube-controller-manager:v1.17.3 kube-scheduler:v1.17.3 coredns:1.6.5 etcd:3.4.3-0 pause:3.1)for imageName in $&#123;images[@]&#125; ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageNamedone mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 添加一个网络在master node将下面抽取成一个yml文件执行kubectl apply -f xxxx.yml ---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [&#x27;NET_ADMIN&#x27;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: &#x27;RunAsAny&#x27;---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelrules: - apiGroups: [&#x27;extensions&#x27;] resources: [&#x27;podsecuritypolicies&#x27;] verbs: [&#x27;use&#x27;] resourceNames: [&#x27;psp.flannel.unprivileged&#x27;] - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | &#123; &quot;name&quot;: &quot;cbr0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ &#123; &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: &#123; &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true &#125; &#125;, &#123; &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: &#123; &quot;portMappings&quot;: true &#125; &#125; ] &#125; net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - amd64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - arm64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - arm hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - ppc64le hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - s390x hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 在node 执行加入master kubeadm join 10.0.2.5:6443 --token vi3lq8.vze5hodae0oufto2 \\ --discovery-token-ca-cert-hash sha256:25518d08b8a03076f808200b2de973b089c7b04b877d9fbc51a9c5139d141180 至此, k8s 集群的基础环境搭建完成。接下来会介绍如何搭建kubesphere。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"https://voox.cc/tags/k8s/"},{"name":"k8s-cluster","slug":"k8s-cluster","permalink":"https://voox.cc/tags/k8s-cluster/"}]},{"title":"Sentinel 的使用","slug":"Sentinel-的使用","date":"2020-10-09T09:30:37.000Z","updated":"2020-10-09T09:38:54.347Z","comments":true,"path":"2020/10/09/Sentinel-的使用/","link":"","permalink":"https://voox.cc/2020/10/09/Sentinel-%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"引入 Sentinel 依赖如果您的应用使用了 Maven，则在 pom.xml 文件中加入以下代码即可：&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 下面这个例子就是一个最简单的使用 Sentinel 的例子: @SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceApplication.class, args); &#125;&#125;@Servicepublic class TestService &#123; @SentinelResource(value = &quot;sayHello&quot;) public String sayHello(String name) &#123; return &quot;Hello, &quot; + name; &#125;&#125;@RestControllerpublic class TestController &#123; @Autowired private TestService service; @GetMapping(value = &quot;/hello/&#123;name&#125;&quot;) public String apiHello(@PathVariable String name) &#123; return service.sayHello(name); &#125;&#125; @SentinelResource 注解用来标识资源是否被限流、降级。上述例子上该注解的属性 sayHello 表示资源名。 @SentinelResource 还提供了其它额外的属性如 blockHandler，blockHandlerClass，fallback 用于表示限流或降级的操作（注意有方法签名要求），更多内容可以参考 Sentinel 注解支持文档。若不配置 blockHandler、fallback 等函数，则被流控降级时方法会直接抛出对应的 BlockException；若方法未定义 throws BlockException 则会被 JVM 包装一层 UndeclaredThrowableException。 注：一般推荐将 @SentinelResource 注解加到服务实现上，而在 Web 层直接使用 Spring Cloud Alibaba 自带的 Web 埋点适配。Sentinel Web 适配同样支持配置自定义流控处理逻辑，参考 相关文档。 Sentinel 控制台Sentinel 控制台提供一个轻量级的控制台，它提供机器发现、单机资源实时监控、集群资源汇总，以及规则管理的功能。您只需要对应用进行简单的配置，就可以使用这些功能。 注意: 集群资源汇总仅支持 500 台以下的应用集群，有大概 1 - 2 秒的延时。 Figure 1. Sentinel Dashboard开启该功能需要3个步骤： 获取控制台您可以从 release 页面 下载最新版本的控制台 jar 包。 您也可以从最新版本的源码自行构建 Sentinel 控制台： 下载 控制台 工程 使用以下命令将代码打包成一个 fat jar: mvn clean package 启动控制台Sentinel 控制台是一个标准的 Spring Boot 应用，以 Spring Boot 的方式运行 jar 包即可。 java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 如若8080端口冲突，可使用 -Dserver.port=新端口 进行设置 配置控制台信息application.yml spring: cloud: sentinel: transport: port: 8719 dashboard: localhost:8080 这里的 spring.cloud.sentinel.transport.port 端口配置会在应用对应的机器上启动一个 Http Server，该 Server 会与 Sentinel 控制台做交互。比如 Sentinel 控制台添加了一个限流规则，会把规则数据 push 给这个 Http Server 接收，Http Server 再将规则注册到 Sentinel 中。 更多 Sentinel 控制台的使用及问题参考： Sentinel 控制台文档 以及 Sentinel FAQ Feign 支持Sentinel 适配了 Feign 组件。如果想使用，除了引入 spring-cloud-starter-alibaba-sentinel 的依赖外还需要 2 个步骤： 配置文件打开 Sentinel 对 Feign 的支持：feign.sentinel.enabled=true 加入 spring-cloud-starter-openfeign 依赖使 Sentinel starter 中的自动化配置类生效： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 这是一个 FeignClient 的简单使用示例： @FeignClient(name = &quot;service-provider&quot;, fallback = EchoServiceFallback.class, configuration = FeignConfiguration.class)public interface EchoService &#123; @RequestMapping(value = &quot;/echo/&#123;str&#125;&quot;, method = RequestMethod.GET) String echo(@PathVariable(&quot;str&quot;) String str);&#125;class FeignConfiguration &#123; @Bean public EchoServiceFallback echoServiceFallback() &#123; return new EchoServiceFallback(); &#125;&#125;class EchoServiceFallback implements EchoService &#123; @Override public String echo(@PathVariable(&quot;str&quot;) String str) &#123; return &quot;echo fallback&quot;; &#125;&#125; Feign 对应的接口中的资源名策略定义：httpmethod:protocol://requesturl。@FeignClient 注解中的所有属性，Sentinel 都做了兼容EchoService 接口中方法 echo 对应的资源名为 GET:http://service-provider/echo/{str}。","categories":[],"tags":[]},{"title":"从0到1手写一个RPC实现","slug":"从0到1手写一个RPC实现","date":"2020-10-08T05:35:36.000Z","updated":"2020-10-09T08:23:11.683Z","comments":true,"path":"2020/10/08/从0到1手写一个RPC实现/","link":"","permalink":"https://voox.cc/2020/10/08/%E4%BB%8E0%E5%88%B01%E6%89%8B%E5%86%99%E4%B8%80%E4%B8%AARPC%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"计划从0到1手写一个RPC的实现我准备把具体实现和想法记录下来，对学习其他RPC框架有一个参考和比较。可能用到的技术栈： spring netty kryo, protobuf, thrift, hessian (序列化和反序列化) zookeeper, etcd, redis (服务注册和发现)","categories":[],"tags":[{"name":"rpc","slug":"rpc","permalink":"https://voox.cc/tags/rpc/"},{"name":"java","slug":"java","permalink":"https://voox.cc/tags/java/"}]},{"title":"AQS","slug":"AQS","date":"2020-10-08T04:48:48.000Z","updated":"2020-10-09T07:40:34.131Z","comments":true,"path":"2020/10/08/AQS/","link":"","permalink":"https://voox.cc/2020/10/08/AQS/","excerpt":"","text":"1 ReentrantLock1.1 ReentrantLock特性概览ReentrantLock意思为可重入锁，指的是一个线程能够对一个临界资源重复加锁。为了帮助大家更好地理解ReentrantLock的特性，我们先将ReentrantLock跟常用的Synchronized进行比较，其特性如下: // **************************Synchronized的使用方式**************************// 1.用于代码块synchronized (this) &#123;&#125;// 2.用于对象synchronized (object) &#123;&#125;// 3.用于方法public synchronized void test () &#123;&#125;// 4.可重入for (int i = 0; i &lt; 100; i++) &#123; synchronized (this) &#123;&#125;&#125;// **************************ReentrantLock的使用方式**************************public void test () throw Exception &#123; // 1.初始化选择公平锁、非公平锁 ReentrantLock lock = new ReentrantLock(true); // 2.可用于代码块 lock.lock(); try &#123; try &#123; // 3.支持多种加锁方式，比较灵活; 具有可重入特性 if(lock.tryLock(100, TimeUnit.MILLISECONDS))&#123; &#125; &#125; finally &#123; // 4.手动释放锁 lock.unlock() &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 1.2 ReentrantLock与AQS的关联非公平锁源码中的加锁流程如下： // java.util.concurrent.locks.ReentrantLock#NonfairSync// 非公平锁static final class NonfairSync extends Sync &#123; ... final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; ...&#125; 这块代码的含义为： 若通过CAS设置变量State（同步状态）成功，也就是获取锁成功，则将当前线程设置为独占线程。 若通过CAS设置变量State（同步状态）失败，也就是获取锁失败，则进入Acquire方法进行后续处理。 第一步很好理解，但第二步获取锁失败后，后续的处理策略是怎么样的呢？这块可能会有以下思考： 某个线程获取锁失败的后续流程是什么呢？有以下两种可能：(1) 将当前线程获锁结果设置为失败，获取锁流程结束。这种设计会极大降低系统的并发度，并不满足我们实际的需求。所以就需要下面这种流程，也就是AQS框架的处理流程。 (2) 存在某种排队等候机制，线程继续等待，仍然保留获取锁的可能，获取锁流程仍在继续。 对于问题1的第二种情况，既然说到了排队等候机制，那么就一定会有某种队列形成，这样的队列是什么数据结构呢？ 处于排队等候机制中的线程，什么时候可以有机会获取锁呢？ 如果处于排队等候机制中的线程一直无法获取锁，还是需要一直等待吗，还是有别的策略来解决这一问题？ 带着非公平锁的这些问题，再看下公平锁源码中获锁的方式： // java.util.concurrent.locks.ReentrantLock#FairSyncstatic final class FairSync extends Sync &#123; ... final void lock() &#123; acquire(1); &#125; ...&#125; 看到这块代码，我们可能会存在这种疑问：Lock函数通过Acquire方法进行加锁，但是具体是如何加锁的呢？ 结合公平锁和非公平锁的加锁流程，虽然流程上有一定的不同，但是都调用了Acquire方法，而Acquire方法是FairSync和UnfairSync的父类AQS中的核心方法。 对于上边提到的问题，其实在ReentrantLock类源码中都无法解答，而这些问题的答案，都是位于Acquire方法所在的类AbstractQueuedSynchronizer中，也就是本文的核心——AQS。 2 AQS首先，我们通过下面的架构图来整体了解一下AQS框架： 上图中有颜色的为Method，无颜色的为Attribution。 总的来说，AQS框架共分为五层，自上而下由浅入深，从AQS对外暴露的API到底层基础数据。 当有自定义同步器接入时，只需重写第一层所需要的部分方法即可，不需要关注底层具体的实现流程。当自定义同步器进行加锁或者解锁操作时，先经过第一层的API进入AQS内部方法，然后经过第二层进行锁的获取，接着对于获取锁失败的流程，进入第三层和第四层的等待队列处理，而这些处理方式均依赖于第五层的基础数据提供层。 下面我们会从整体到细节，从流程到方法逐一剖析AQS框架，主要分析过程如下： 2.1 原理概览AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。 主要原理图如下： AQS使用一个Volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。 2.1.1 AQS数据结构先来看下AQS中最基本的数据结构——Node，Node即为上面CLH变体队列中的节点。 解释一下几个方法和属性值的含义： 线程两种锁的模式： waitStatus有下面几个枚举值： 2.1.2 同步状态State在了解数据结构后，接下来了解一下AQS的同步状态——State。AQS中维护了一个名为state的字段，意为同步状态，是由Volatile修饰的，用于展示当前临界资源的获锁情况。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate volatile int state; 下面提供了几个访问这个字段的方法： 这几个方法都是Final修饰的，说明子类中无法重写它们。我们可以通过修改State字段表示的同步状态来实现多线程的独占模式和共享模式（加锁过程）。 对于我们自定义的同步工具，需要自定义获取同步状态和释放状态的方式，也就是AQS架构图中的第一层：API层。 2.2 AQS重要方法与ReentrantLock的关联从架构图中可以得知，AQS提供了大量用于自定义同步器实现的Protected方法。自定义同步器实现的相关方法也只是为了通过修改State字段来实现多线程的独占模式或者共享模式。自定义同步器需要实现以下方法（ReentrantLock需要实现的方法如下，并不是全部）： 一般来说，自定义同步器要么是独占方式，要么是共享方式，它们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。ReentrantLock是独占锁，所以实现了tryAcquire-tryRelease。 以非公平锁为例，这里主要阐述一下非公平锁与AQS之间方法的关联之处，具体每一处核心方法的作用会在文章后面详细进行阐述。 为了帮助大家理解ReentrantLock和AQS之间方法的交互过程，以非公平锁为例，我们将加锁和解锁的交互流程单独拎出来强调一下，以便于对后续内容的理解。 加锁： 通过ReentrantLock的加锁方法Lock进行加锁操作。 会调用到内部类Sync的Lock方法，由于Sync#lock是抽象方法，根据ReentrantLock初始化选择的公平锁和非公平锁，执行相关内部类的Lock方法，本质上都会执行AQS的Acquire方法。 AQS的Acquire方法会执行tryAcquire方法，但是由于tryAcquire需要自定义同步器实现，因此执行了ReentrantLock中的tryAcquire方法，由于ReentrantLock是通过公平锁和非公平锁内部类实现的tryAcquire方法，因此会根据锁类型不同，执行不同的tryAcquire。 tryAcquire是获取锁逻辑，获取失败后，会执行框架AQS的后续逻辑，跟ReentrantLock自定义同步器无关。解锁： 通过ReentrantLock的解锁方法Unlock进行解锁。 Unlock会调用内部类Sync的Release方法，该方法继承于AQS。 Release中会调用tryRelease方法，tryRelease需要自定义同步器实现，tryRelease只在ReentrantLock中的Sync实现，因此可以看出，释放锁的过程，并不区分是否为公平锁。 释放成功后，所有处理由AQS框架完成，与自定义同步器无关。 通过上面的描述，大概可以总结出ReentrantLock加锁解锁时API层核心方法的映射关系。 2.3 通过ReentrantLock理解AQSReentrantLock中公平锁和非公平锁在底层是相同的，这里以非公平锁为例进行分析。 在非公平锁中，有一段这样的代码： // java.util.concurrent.locks.ReentrantLockstatic final class NonfairSync extends Sync &#123; ... final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; ...&#125; 看一下这个Acquire是怎么写的： // java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 再看一下tryAcquire方法： // java.util.concurrent.locks.AbstractQueuedSynchronizerprotected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; 可以看出，这里只是AQS的简单实现，具体获取锁的实现方法是由各自的公平锁和非公平锁单独实现的（以ReentrantLock为例）。如果该方法返回了True，则说明当前线程获取锁成功，就不用往后执行了；如果获取失败，就需要加入到等待队列中。下面会详细解释线程是何时以及怎样被加入进等待队列中的。 2.3.1 线程加入等待队列2.3.1.1 加入队列的时机当执行Acquire(1)时，会通过tryAcquire获取锁。在这种情况下，如果获取锁失败，就会调用addWaiter加入到等待队列中去。 2.3.1.2 如何加入队列获取锁失败后，会执行addWaiter(Node.EXCLUSIVE)加入等待队列，具体实现方法如下： // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125;private final boolean compareAndSetTail(Node expect, Node update) &#123; return unsafe.compareAndSwapObject(this, tailOffset, expect, update);&#125; 主要的流程如下： 通过当前的线程和锁模式新建一个节点。 Pred指针指向尾节点Tail。 将New中Node的Prev指针指向Pred。 通过compareAndSetTail方法，完成尾节点的设置。这个方法主要是对tailOffset和Expect进行比较，如果tailOffset的Node和Expect的Node地址是相同的，那么设置Tail的值为Update的值。 // java.util.concurrent.locks.AbstractQueuedSynchronizerstatic &#123; try &#123; stateOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;state&quot;)); headOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;head&quot;)); tailOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.class.getDeclaredField(&quot;tail&quot;)); waitStatusOffset = unsafe.objectFieldOffset(Node.class.getDeclaredField(&quot;waitStatus&quot;)); nextOffset = unsafe.objectFieldOffset(Node.class.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125; 从AQS的静态代码块可以看出，都是获取一个对象的属性相对于该对象在内存当中的偏移量，这样我们就可以根据这个偏移量在对象内存当中找到这个属性。tailOffset指的是tail对应的偏移量，所以这个时候会将new出来的Node置为当前队列的尾节点。同时，由于是双向链表，也需要将前一个节点指向尾节点。 如果Pred指针是Null（说明等待队列中没有元素），或者当前Pred指针和Tail指向的位置不同（说明被别的线程已经修改），就需要看一下Enq的方法。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 如果没有被初始化，需要进行初始化一个头结点出来。但请注意，初始化的头结点并不是当前线程节点，而是调用了无参构造函数的节点。如果经历了初始化或者并发导致队列中有元素，则与之前的方法相同。其实，addWaiter就是一个在双端链表添加尾节点的操作，需要注意的是，双端链表的头结点是一个无参构造函数的头结点。 总结一下，线程获取锁的时候，过程大体如下： 当没有线程获取到锁时，线程1获取锁成功。 线程2申请锁，但是锁被线程1占有。 如果再有线程要获取锁，依次在队列中往后排队即可。回到上边的代码，hasQueuedPredecessors是公平锁加锁时判断等待队列中是否存在有效节点的方法。如果返回False，说明当前线程可以争取共享资源；如果返回True，说明队列中存在有效节点，当前线程必须加入到等待队列中。 // java.util.concurrent.locks.ReentrantLockpublic final boolean hasQueuedPredecessors() &#123; // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 看到这里，我们理解一下h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());为什么要判断的头结点的下一个节点？第一个节点储存的数据是什么？ 双向链表中，第一个节点为虚节点，其实并不存储任何信息，只是占位。真正的第一个有数据的节点，是在第二个节点开始的。当h != t时： 如果(s = h.next) == null，等待队列正在有线程进行初始化，但只是进行到了Tail指向Head，没有将Head指向Tail，此时队列中有元素，需要返回True（这块具体见下边代码分析）。 如果(s = h.next) != null，说明此时队列中至少有一个有效节点。如果此时s.thread == Thread.currentThread()，说明等待队列的第一个有效节点中的线程与当前线程相同，那么当前线程是可以获取资源的；如果s.thread != Thread.currentThread()，说明等待队列的第一个有效节点线程与当前线程不同，当前线程必须加入进等待队列。 // java.util.concurrent.locks.AbstractQueuedSynchronizer#enqif (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head;&#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125;&#125; 节点入队不是原子操作，所以会出现短暂的head != tail，此时Tail指向最后一个节点，而且Tail指向Head。如果Head没有指向Tail（可见5、6、7行），这种情况下也需要将相关线程加入队列中。所以这块代码是为了解决极端情况下的并发问题。 2.3.1.3 等待队列中线程出队列时机回到最初的源码： // java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 上文解释了addWaiter方法，这个方法其实就是把对应的线程以Node的数据结构形式加入到双端队列里，返回的是一个包含该线程的Node。而这个Node会作为参数，进入到acquireQueued方法中。acquireQueued方法可以对排队中的线程进行“获锁”操作。 总的来说，一个线程获取锁失败了，被放入等待队列，acquireQueued会把放入队列中的线程不断去获取锁，直到获取成功或者不再需要获取（中断）。 下面我们从“何时出队列？”和“如何出队列？”两个方向来分析一下acquireQueued源码： // java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; // 标记是否成功拿到资源 boolean failed = true; try &#123; // 标记等待过程中是否中断过 boolean interrupted = false; // 开始自旋，要么获取锁，要么中断 for (;;) &#123; // 获取当前节点的前驱节点 final Node p = node.predecessor(); // 如果p是头结点，说明当前节点在真实数据队列的首部，就尝试获取锁（别忘了头结点是虚节点） if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取锁成功，头指针移动到当前node setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 说明p为头节点且当前没有获取到锁（可能是非公平锁被抢占了）或者是p不为头结点，这个时候就要判断当前node是否要被阻塞（被阻塞条件：前驱节点的waitStatus为-1），防止无限循环浪费资源。具体两个方法下面细细分析 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 注：setHead方法是把当前节点置为虚节点，但并没有修改waitStatus，因为它是一直需要用的数据。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void setHead(Node node) &#123; head = node; node.thread = null; node.prev = null;&#125;// java.util.concurrent.locks.AbstractQueuedSynchronizer// 靠前驱节点判断当前线程是否应该被阻塞private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 获取头结点的节点状态 int ws = pred.waitStatus; // 说明头结点处于唤醒状态 if (ws == Node.SIGNAL) return true; // 通过枚举值我们知道waitStatus&gt;0是取消状态 if (ws &gt; 0) &#123; do &#123; // 循环向前查找取消节点，把取消节点从队列中剔除 node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 设置前任节点等待状态为SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt主要用于挂起当前线程，阻塞调用栈，返回当前线程的中断状态。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 上述方法的流程图如下： 从上图可以看出，跳出当前循环的条件是当“前置节点是头结点，且当前线程获取锁成功”。为了防止因死循环导致CPU资源被浪费，我们会判断前置节点的状态来决定是否要将当前线程挂起，具体挂起流程用流程图表示如下（shouldParkAfterFailedAcquire流程）： 从队列中释放节点的疑虑打消了，那么又有新问题了： shouldParkAfterFailedAcquire中取消节点是怎么生成的呢？什么时候会把一个节点的waitStatus设置为-1？ 是在什么时间释放节点通知到被挂起的线程呢？ 2.3.2 CANCELLED状态节点生成acquireQueued方法中的Finally代码： // java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; ... for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; ... failed = false; ... &#125; ... &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 通过cancelAcquire方法，将Node的状态标记为CANCELLED。接下来，我们逐行来分析这个方法的原理： // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void cancelAcquire(Node node) &#123; // 将无效节点过滤 if (node == null) return; // 设置该节点不关联任何线程，也就是虚节点 node.thread = null; Node pred = node.prev; // 通过前驱节点，跳过取消状态的node while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取过滤后的前驱节点的后继节点 Node predNext = pred.next; // 把当前node的状态设置为CANCELLED node.waitStatus = Node.CANCELLED; // 如果当前节点是尾节点，将从后往前的第一个非取消状态的节点设置为尾节点 // 更新失败的话，则进入else，如果更新成功，将tail的后继节点设置为null if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; int ws; // 如果当前节点不是head的后继节点，1:判断当前节点前驱节点的是否为SIGNAL，2:如果不是，则把前驱节点设置为SINGAL看是否成功 // 如果1和2中有一个为true，再判断当前节点的线程是否为null // 如果上述条件都满足，把当前节点的前驱节点的后继指针指向当前节点的后继节点 if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果当前节点是head的后继节点，或者上述条件不满足，那就唤醒当前节点的后继节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 当前的流程： 获取当前节点的前驱节点，如果前驱节点的状态是CANCELLED，那就一直往前遍历，找到第一个waitStatus &lt;= 0的节点，将找到的Pred节点和当前Node关联，将当前Node设置为CANCELLED。 根据当前节点的位置，考虑以下三种情况： (1) 当前节点是尾节点。 (2) 当前节点是Head的后继节点。 (3) 当前节点不是Head的后继节点，也不是尾节点。 根据上述第二条，我们来分析每一种情况的流程。 当前节点是尾节点。 当前节点是Head的后继节点。 当前节点不是Head的后继节点，也不是尾节点。 通过上面的流程，我们对于CANCELLED节点状态的产生和变化已经有了大致的了解，但是为什么所有的变化都是对Next指针进行了操作，而没有对Prev指针进行操作呢？什么情况下会对Prev指针进行操作？ 执行cancelAcquire的时候，当前节点的前置节点可能已经从队列中出去了（已经执行过Try代码块中的shouldParkAfterFailedAcquire方法了），如果此时修改Prev指针，有可能会导致Prev指向另一个已经移除队列的Node，因此这块变化Prev指针不安全。 shouldParkAfterFailedAcquire方法中，会执行下面的代码，其实就是在处理Prev指针。shouldParkAfterFailedAcquire是获取锁失败的情况下才会执行，进入该方法后，说明共享资源已被获取，当前节点之前的节点都不会出现变化，因此这个时候变更Prev指针比较安全。 do &#123; node.prev = pred = pred.prev;&#125; while (pred.waitStatus &gt; 0); 2.3.3 如何解锁我们已经剖析了加锁过程中的基本流程，接下来再对解锁的基本流程进行分析。由于ReentrantLock在解锁的时候，并不区分公平锁和非公平锁，所以我们直接看解锁的源码： // java.util.concurrent.locks.ReentrantLockpublic void unlock() &#123; sync.release(1);&#125; 可以看到，本质释放锁的地方，是通过框架来完成的。 // java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 在ReentrantLock里面的公平锁和非公平锁的父类Sync定义了可重入锁的释放锁机制。 // java.util.concurrent.locks.ReentrantLock.Sync// 方法返回当前锁是不是没有被线程持有protected final boolean tryRelease(int releases) &#123; // 减少可重入次数 int c = getState() - releases; // 当前线程不是持有锁的线程，抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 如果持有线程全部释放，将当前独占锁所有线程设置为null，并更新state if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 我们来解释下述源码： // java.util.concurrent.locks.AbstractQueuedSynchronizerpublic final boolean release(int arg) &#123; // 上边自定义的tryRelease如果返回true，说明该锁没有被任何线程持有 if (tryRelease(arg)) &#123; // 获取头结点 Node h = head; // 头结点不为空并且头结点的waitStatus不是初始化节点情况，解除线程挂起状态 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 这里的判断条件为什么是h != null &amp;&amp; h.waitStatus != 0？ h == null Head还没初始化。初始情况下，head == null，第一个节点入队，Head会被初始化一个虚拟节点。所以说，这里如果还没来得及入队，就会出现head == null 的情况。 h != null &amp;&amp; waitStatus == 0 表明后继节点对应的线程仍在运行中，不需要唤醒。 h != null &amp;&amp; waitStatus &lt; 0 表明后继节点可能被阻塞了，需要唤醒。 再看一下unparkSuccessor方法： // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void unparkSuccessor(Node node) &#123; // 获取头结点waitStatus int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); // 获取当前节点的下一个节点 Node s = node.next; // 如果下个节点是null或者下个节点被cancelled，就找到队列最开始的非cancelled的节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 就从尾部节点开始找，到队首，找到队列第一个waitStatus&lt;0的节点。 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; // 如果当前节点的下个节点不为空，而且状态&lt;=0，就把当前节点unpark if (s != null) LockSupport.unpark(s.thread);&#125; 为什么要从后往前找第一个非Cancelled的节点呢？原因如下。 之前的addWaiter方法： // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 我们从这里可以看到，节点入队并不是原子操作，也就是说，node.prev = pred; compareAndSetTail(pred, node) 这两个地方可以看作Tail入队的原子操作，但是此时pred.next = node;还没执行，如果这个时候执行了unparkSuccessor方法，就没办法从前往后找了，所以需要从后往前找。还有一点原因，在产生CANCELLED状态节点的时候，先断开的是Next指针，Prev指针并未断开，因此也是必须要从后往前遍历才能够遍历完全部的Node。 综上所述，如果是从前往后找，由于极端情况下入队的非原子操作和CANCELLED节点产生过程中断开Next指针的操作，可能会导致无法遍历所有的节点。所以，唤醒对应的线程后，对应的线程就会继续往下执行。继续执行acquireQueued方法以后，中断如何处理？ 2.3.4 中断恢复后的执行流程唤醒后，会执行return Thread.interrupted();，这个函数返回的是当前执行线程的中断状态，并清除。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 再回到acquireQueued代码，当parkAndCheckInterrupt返回True或者False的时候，interrupted的值不同，但都会执行下次循环。如果这个时候获取锁成功，就会把当前interrupted返回。 // java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 如果acquireQueued为True，就会执行selfInterrupt方法。 // java.util.concurrent.locks.AbstractQueuedSynchronizerstatic void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; 该方法其实是为了中断线程。但为什么获取了锁以后还要中断线程呢？这部分属于Java提供的协作式中断知识内容，感兴趣同学可以查阅一下。这里简单介绍一下： 当中断线程被唤醒时，并不知道被唤醒的原因，可能是当前线程在等待中被中断，也可能是释放了锁以后被唤醒。因此我们通过Thread.interrupted()方法检查中断标记（该方法返回了当前线程的中断状态，并将当前线程的中断标识设置为False），并记录下来，如果发现该线程被中断过，就再中断一次。 线程在等待资源的过程中被唤醒，唤醒后还是会不断地去尝试获取锁，直到抢到锁为止。也就是说，在整个流程中，并不响应中断，只是记录中断记录。最后抢到锁返回了，那么如果被中断过的话，就需要补充一次中断。 这里的处理方式主要是运用线程池中基本运作单元Worder中的runWorker，通过Thread.interrupted()进行额外的判断处理，感兴趣的同学可以看下ThreadPoolExecutor源码。 2.3.5 小结Q：某个线程获取锁失败的后续流程是什么呢？A：存在某种排队等候机制，线程继续等待，仍然保留获取锁的可能，获取锁流程仍在继续。 Q：既然说到了排队等候机制，那么就一定会有某种队列形成，这样的队列是什么数据结构呢？A：是CLH变体的FIFO双端队列。 Q：处于排队等候机制中的线程，什么时候可以有机会获取锁呢？A：可以详细看下2.3.1.3小节。 Q：如果处于排队等候机制中的线程一直无法获取锁，需要一直等待么？还是有别的策略来解决这一问题？A：线程所在节点的状态会变成取消状态，取消状态的节点会从队列中释放，具体可见2.3.2小节。 Q：Lock函数通过Acquire方法进行加锁，但是具体是如何加锁的呢？A：AQS的Acquire会调用tryAcquire方法，tryAcquire由各个自定义同步器实现，通过tryAcquire完成加锁过程。 3 AQS应用3.1 ReentrantLock的可重入应用ReentrantLock的可重入性是AQS很好的应用之一，在了解完上述知识点以后，我们很容易得知ReentrantLock实现可重入的方法。在ReentrantLock里面，不管是公平锁还是非公平锁，都有一段逻辑。 公平锁： // java.util.concurrent.locks.ReentrantLock.FairSync#tryAcquireif (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125;&#125;else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true;&#125; 非公平锁： // java.util.concurrent.locks.ReentrantLock.Sync#nonfairTryAcquireif (c == 0) &#123; if (compareAndSetState(0, acquires))&#123; setExclusiveOwnerThread(current); return true; &#125;&#125;else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true;&#125; 从上面这两段都可以看到，有一个同步状态State来控制整体可重入的情况。State是Volatile修饰的，用于保证一定的可见性和有序性。 // java.util.concurrent.locks.AbstractQueuedSynchronizerprivate volatile int state; 接下来看State这个字段主要的过程： State初始化的时候为0，表示没有任何线程持有锁。 当有线程持有该锁时，值就会在原来的基础上+1，同一个线程多次获得锁是，就会多次+1，这里就是可重入的概念。 解锁也是对这个字段-1，一直到0，此线程对锁释放。 3.2 JUC中的应用场景除了上边ReentrantLock的可重入性的应用，AQS作为并发编程的框架，为很多其他同步工具提供了良好的解决方案。下面列出了JUC中的几种同步工具，大体介绍一下AQS的应用场景： 3.3 自定义同步工具了解AQS基本原理以后，按照上面所说的AQS知识点，自己实现一个同步工具。 public class LeeLock &#123; private static class Sync extends AbstractQueuedSynchronizer &#123; @Override protected boolean tryAcquire (int arg) &#123; return compareAndSetState(0, 1); &#125; @Override protected boolean tryRelease (int arg) &#123; setState(0); return true; &#125; @Override protected boolean isHeldExclusively () &#123; return getState() == 1; &#125; &#125; private Sync sync = new Sync(); public void lock () &#123; sync.acquire(1); &#125; public void unlock () &#123; sync.release(1); &#125;&#125; 通过我们自己定义的Lock完成一定的同步功能。 public class LeeMain &#123; static int count = 0; static LeeLock leeLock = new LeeLock(); public static void main (String[] args) throws InterruptedException &#123; Runnable runnable = new Runnable() &#123; @Override public void run () &#123; try &#123; leeLock.lock(); for (int i = 0; i &lt; 10000; i++) &#123; count++; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; leeLock.unlock(); &#125; &#125; &#125;; Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(count); &#125;&#125; 上述代码每次运行结果都会是20000。通过简单的几行代码就能实现同步功能，这就是AQS的强大之处。 本文章转载自： https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html","categories":[],"tags":[]},{"title":"update by partition","slug":"update-by-partition","date":"2019-03-01T07:25:19.000Z","updated":"2019-03-01T07:29:11.238Z","comments":true,"path":"2019/03/01/update-by-partition/","link":"","permalink":"https://voox.cc/2019/03/01/update-by-partition/","excerpt":"","text":"sql update top 5 SELECT ROW_NUMBER() OVER (PARTITION BY col1, col2 ORDER BY x desc) AS r, t.id FROM some_table t where condition = &#x27;xxx&#x27;) Awhere A.r &lt; 5;","categories":[],"tags":[{"name":"sql","slug":"sql","permalink":"https://voox.cc/tags/sql/"},{"name":"postgresql","slug":"postgresql","permalink":"https://voox.cc/tags/postgresql/"}]},{"title":"Docker中exec和attach区别","slug":"Docker中exec和attach区别","date":"2018-08-08T02:57:20.000Z","updated":"2019-01-31T06:11:40.735Z","comments":true,"path":"2018/08/08/Docker中exec和attach区别/","link":"","permalink":"https://voox.cc/2018/08/08/Docker%E4%B8%ADexec%E5%92%8Cattach%E5%8C%BA%E5%88%AB/","excerpt":"","text":"docker attach 执行后会进入到 container 中docker exec 执行后，命令执行返回值并显示到宿主机中。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"}]},{"title":"Docker alias","slug":"docker-alias","date":"2018-02-11T06:07:03.000Z","updated":"2019-01-31T06:14:34.507Z","comments":true,"path":"2018/02/11/docker-alias/","link":"","permalink":"https://voox.cc/2018/02/11/docker-alias/","excerpt":"","text":"Docker alias and functionGet latest container IDalias dl=&quot;docker ps -l -q&quot;Get container processalias dps=&quot;docker ps&quot;Get process included stop containeralias dpa=&quot;docker ps -a&quot;Get imagesalias di=&quot;docker images&quot;Get container IPalias dip=&quot;docker inspect --format &#x27;&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;&#x27;&quot;Run deamonized container, e.g., $dkd base /bin/echo helloalias dkd=&quot;docker run -d -P&quot;Run interactive container, e.g., $dki base /bin/bashalias dki=&quot;docker run -i -t -P&quot;Execute interactive container, e.g., $dex base /bin/bashalias dex=&quot;docker exec -i -t&quot;Stop all containersdstop() &#123; docker stop $(docker ps -a -q); &#125;Remove all containersdrm() &#123; docker rm $(docker ps -a -q); &#125;Stop and Remove all containersalias drmf=&#x27;docker stop $(docker ps -a -q) &amp;&amp; docker rm $(docker ps -a -q)&#x27;Remove all imagesdri() &#123; docker rmi $(docker images -q); &#125;Dockerfile build, e.g., $dbu tcnksm/testdbu() &#123; docker build -t=$1 .; &#125;Show all alias related dockerdalias() &#123; alias | grep &#x27;docker&#x27; | sed &quot;s/^([^=])=(.)/\\1 =&gt; \\2/&quot;| sed &quot;s/[&#x27;|&#x27;]//g&quot; | sort; &#125;Bash into running containerdbash() &#123; docker exec -it $(docker ps -aqf &quot;name=$1&quot;) bash; &#125;","categories":[],"tags":[{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]},{"title":"Docker常用命令及使用","slug":"Docker常用命令及使用","date":"2017-11-20T05:45:18.000Z","updated":"2019-02-01T02:48:19.522Z","comments":true,"path":"2017/11/20/Docker常用命令及使用/","link":"","permalink":"https://voox.cc/2017/11/20/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E4%BD%BF%E7%94%A8/","excerpt":"","text":"以下是在 centos 系统下执行 sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.reposudo yum makecache fastsudo yum install docker-cevi /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot; : [ &quot;https://xxxx.mirror.aliyuncs.com&quot; ], &quot;insecure-registries&quot; : [ &quot;registry.mirrors.aliyuncs.com&quot; ], &quot;debug&quot; : true, &quot;experimental&quot; : true&#125;sudo systemctl start docker","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]}],"categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://voox.cc/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"https://voox.cc/tags/k8s/"},{"name":"k8s-cluster","slug":"k8s-cluster","permalink":"https://voox.cc/tags/k8s-cluster/"},{"name":"rpc","slug":"rpc","permalink":"https://voox.cc/tags/rpc/"},{"name":"java","slug":"java","permalink":"https://voox.cc/tags/java/"},{"name":"sql","slug":"sql","permalink":"https://voox.cc/tags/sql/"},{"name":"postgresql","slug":"postgresql","permalink":"https://voox.cc/tags/postgresql/"},{"name":"shell","slug":"shell","permalink":"https://voox.cc/tags/shell/"}]}